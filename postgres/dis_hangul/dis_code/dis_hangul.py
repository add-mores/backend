import os
import re
import pandas as pd
from konlpy.tag import Okt
from sqlalchemy import create_engine, text
from sqlalchemy.dialects.postgresql import TEXT, JSONB
from dotenv import load_dotenv

# 1) í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
DATABASE_URL = os.getenv("DATABASE_URL")
if not DATABASE_URL:
    raise RuntimeError("DATABASE_URLì´ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤")

# 2) DB ì—°ê²°
engine = create_engine(DATABASE_URL)

# 3) ì´ì „ ê²°ê³¼ í…Œì´ë¸” ì œê±°
with engine.begin() as conn:
    conn.execute(text("DROP TABLE IF EXISTS testdis;"))

# 4) ì›ë³¸ ë°ì´í„° ì½ê¸°
df_raw = pd.read_sql(
    """
    SELECT
      disnm_ko,
      disnm_en,
      dep,
      "def"    AS definition,
      sym      AS symptoms,
      therapy
    FROM disease;
    """,
    engine
)

# 5) í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
okt = Okt()

# 6) í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ (ì˜ë¬¸Â·ìˆ«ì ì¡°í•© í† í°ë„ ì—¬ê¸°ì—)
WHITELIST = {
    "X-ray","CT","MRI","US","Ultrasound","PET","Endoscopy","Colonoscopy",
    "ECG","EKG","EEG","BP","HR","CBC","BUN","Antibiotic","Rh","RhD",
    "COVID-19","BRCA","BRCA1","BRCA2","XY","DNA","ARC","Cramp","ABO",
    "ASO","ATP","CA125","CA19-9","CEA","Haw River","PSA","GìŠ¤ìºë‹",
    "ICL","LCP","MTX","Xì„ ","MERRF","N95","XXX","XYY","OTC","PP2","RS",
    "VDT","WPW","Aí˜•","Bí˜•","Oí˜•","Cí˜•","Dí˜•","Gí˜•",
    "18ë²ˆ", "Aìœ í˜•", "Bìœ í˜•", "Cìœ í˜•", "Dìœ í˜•", "Gìœ í˜•",
    "í•­ì²´A", "í•­ì²´B", "í•­ì²´O", "í•­ì²´D", "í•­ì²´G",
    "ì‚¬ë‘ë‹ˆ", "ë‹´ë‚­ì—¼", "ë‹´ì„ì¦", "ê°„ê²½í™”", "ì·Œì¥ì—¼"  # í”í•œ ì˜í•™ ìš©ì–´ ì¶”ê°€
}

# 7) ë¶ˆìš©ì–´ ëª©ë¡ (ì˜í•™ í…ìŠ¤íŠ¸ì—ì„œ ìœ ì˜ë¯¸í•˜ì§€ ì•Šì€ ë‹¨ì–´ë“¤)
STOPWORDS = {
    "í™˜ì", "ì§„ë‹¨", "ì¹˜ë£Œ", "ì¦ìƒ", "ì§ˆí™˜", "ì§ˆë³‘", "ê²½ìš°", "ì´ìƒ", 
    "ê°€ëŠ¥", "ìƒíƒœ", "ë³‘ë³€", "ê²€ì‚¬", "í™•ì¸", "ì¢…ë¥˜", "ì›ì¸", "ë°©ë²•",
    "ê²ƒ", "ë“±", "ìˆ˜", "ì‹œ", "ë‚ ", "ë§", "ë•Œ", "ì¤‘", "ë‚´", "ê±°", "ì§‘",
    "ì•", "ë’¤", "ìœ„", "ì•„ë˜", "ì˜†", "ë…„", "ì›”", "ì¼", "ì‹œê°„", "ë¶„", "ì´ˆ",
    "ì´ìœ ", "ì¸¡ë©´", "í˜„ëŒ€", "ìŒì‹", "ì„­ì·¨", "í¬ê¸°", "ì‹¤ì œ", "ê°œì¸", "ì¼ë°˜",
    "ê´€ë¦¬", "ì¸¡ë©´", "í˜„ëŒ€ì¸", "ê³µê°„", "ëŒ€êµ¬", "ëª¨ë‘", "ì´", "ê·¸", "ì €", "ë‚˜", 
    "ë„ˆ", "ìš°ë¦¬", "ë‹¹ì‹ ", "ìì‹ ", "ëˆ„êµ¬", "ë¬´ì—‡", "ì–´ë””", "ì–¸ì œ", "ì–´ë–»ê²Œ", 
    "ì–´ëŠ", "ì™œ", "ì–¼ë§ˆë‚˜", "ì–¼ë§ˆ", "ë§ì´", "ì ê²Œ", "ë”", "ëœ", "ë§Œí¼", "ì •ë„"
}

# 8) ë³µí•© ëª…ì‚¬ (í˜•íƒœì†Œ ë¶„ì„ì—ì„œ ìª¼ê°œì§ˆ ìˆ˜ ìˆëŠ” ì˜ë¯¸ ìˆëŠ” ë³µí•© ëª…ì‚¬)
COMPOUND_NOUNS = {
    "ë‹´ë‚­ê²°ì„", "ì·Œì¥ì—¼", "ì¥ê²°í•µ", "ì¶©ìˆ˜ì—¼", "ìœ„ê¶¤ì–‘", "ì‹­ì´ì§€ì¥ê¶¤ì–‘", 
    "ê°„ê²½í™”", "ê°„ì—¼", "ì‹ ë¶€ì „", "íë ´", "ê¸°ê´€ì§€ì—¼", "ë‹¹ë‡¨ë³‘", "ê°‘ìƒì„ ì—¼",
    "ê³ í˜ˆì••", "ì €í˜ˆì••", "ê³ ì§€í˜ˆì¦", "ì‹¬ê·¼ê²½ìƒ‰", "ë‡Œì¡¸ì¤‘", "ì¹˜ë§¤", "ê³¨ë‹¤ê³µì¦",
    "ê´€ì ˆì—¼", "ë¥˜ë§ˆí‹°ìŠ¤", "ì²œì‹", "íê²°í•µ", "ë°±í˜ˆë³‘", "ë¹ˆí˜ˆ", "ëŒ€ì¥ì•”", "ìœ ë°©ì•”"
}

# 9) í…ìŠ¤íŠ¸ í´ë¦¬ë‹ í•¨ìˆ˜
def clean_text(txt: str) -> str:
    if not isinstance(txt, str):
        return ""
    # ê´„í˜¸ ì•ˆ ì˜ì–´Â·ìˆ«ì ì œê±°
    txt = re.sub(r"\([^ê°€-í£]*\)", "", txt)
    # ml, %, ë‹¨ë… ìˆ«ì ì œê±° (í•œê¸€ê³¼ ë¶™ì€ ê²½ìš°ëŠ” ë³´ì¡´)
    txt = re.sub(r"(?i)\b\d+ml?%?\b", "", txt)
    txt = re.sub(r"\b\d+\b", "", txt)
    # íŠ¹ìˆ˜ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜
    txt = re.sub(r"[^\wê°€-í£\s]", " ", txt)
    # ì—°ì† ê³µë°± â†’ ë‹¨ì¼
    return re.sub(r"\s+", " ", txt).strip()

# 10) íŠ¹ìˆ˜ íŒ¨í„´ ì¶”ì¶œ í•¨ìˆ˜ (ëª…ì‚¬ í˜•íƒœë¡œ ìœ ì§€)
def extract_special_patterns(text):
    special_tokens = []
    
    # "Aí˜•", "Bìœ í˜•" ë“±ì„ ê·¸ëŒ€ë¡œ ì¶”ì¶œ
    types_regex = re.compile(r"[A-Z][ìœ í˜•]|[A-Z][í˜•]")
    for match in types_regex.finditer(text):
        token = match.group(0)
        if token not in special_tokens:
            special_tokens.append(token)
    
    # "í•­ì²´ A", "í•­ì› B" ë“±ì˜ íŒ¨í„´ ì¶”ì¶œ ë° ì •ê·œí™”
    reverse_types_regex = re.compile(r"(í•­ì²´|í•­ì›|ì¸ì)\s*([A-Z])")
    for match in reverse_types_regex.finditer(text):
        token_type = match.group(1)
        letter = match.group(2)
        token = f"{token_type}{letter}"  # "í•­ì²´A" í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”
        if token not in special_tokens:
            special_tokens.append(token)
    
    # í•œê¸€+í˜•/ìœ í˜•/í•­ì²´/í•­/êµ° íŒ¨í„´ ì¶”ì¶œ
    korean_patterns = [
        r"([ê°€-í£]+í˜•)\b",
        r"([ê°€-í£]+ìœ í˜•)\b",
        r"([ê°€-í£]+í•­ì²´)\b",
        r"([ê°€-í£]+í•­ì›)\b",
        r"([ê°€-í£]+í•­)\b",
        r"([ê°€-í£]+êµ°)\b",
        r"([ê°€-í£]+ì¸ì)\b"
    ]
    
    for pattern in korean_patterns:
        for match in re.finditer(pattern, text):
            token = match.group(1)
            if token not in special_tokens and len(token) >= 2:
                special_tokens.append(token)
    
    return special_tokens

# 11) í•œê¸€ ëª…ì‚¬ í† í° ìœ íš¨ì„± ê²€ì‚¬ (ê°œì„ ë¨)
def is_valid_token(tok: str) -> bool:
    # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ìš°ì„ 
    if tok in WHITELIST:
        return True
    # ë³µí•© ëª…ì‚¬ ìš°ì„ 
    if tok in COMPOUND_NOUNS:
        return True
    # ë¶ˆìš©ì–´ ì œì™¸
    if tok in STOPWORDS:
        return False
    # ìµœì†Œ ê¸¸ì´ ê²€ì‚¬ (ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´ëŠ” ë³´í†µ 2ì ì´ìƒ)
    if len(tok) < 2:
        return False
    # ì˜¨ì „íˆ í•œê¸€ë¡œë§Œ ëœ í† í° (ë˜ëŠ” í•œê¸€+ì˜ë¬¸)
    return bool(re.fullmatch(r"[ê°€-í£A-Za-z]+", tok))

# 12) ì˜í•™ ìš©ì–´ ì¶”ì¶œ ë³´ì¡° í•¨ìˆ˜ (Oktê°€ ë†“ì¹  ìˆ˜ ìˆëŠ” ìš©ì–´ ì¶”ì¶œ)
def extract_medical_terms(text):
    medical_terms = []
    
    # ë³µí•© ëª…ì‚¬ ì¶”ì¶œ
    for term in COMPOUND_NOUNS:
        if term in text and term not in medical_terms:
            medical_terms.append(term)
    
    # ì¶”ê°€ì ì¸ ì˜í•™ ìš©ì–´ íŒ¨í„´ ì¶”ì¶œ (ì˜ˆ: OOì¦, OOë³‘, OOì•” ë“±)
    medical_patterns = [
        r"([ê°€-í£]{1,5}ì¦)\b",  # ë‹¹ë‡¨ì¦, ë¹ˆí˜ˆì¦ ë“±
        r"([ê°€-í£]{1,5}ë³‘)\b",  # íŒŒí‚¨ìŠ¨ë³‘, ì•Œì¸ í•˜ì´ë¨¸ë³‘ ë“±
        r"([ê°€-í£]{1,5}ì—¼)\b",  # ìœ„ì—¼, ë‹´ë‚­ì—¼ ë“±
        r"([ê°€-í£]{1,5}ì•”)\b",  # íì•”, ê°„ì•” ë“±
        r"([ê°€-í£]{1,5}í†µ)\b",  # ë‘í†µ, ë³µí†µ ë“±
        r"([ê°€-í£]{1,5}ë§‰)\b",  # ë§ë§‰, ì ë§‰ ë“±
        r"([ê°€-í£]{1,5}ê³¨)\b",  # ë‘ê°œê³¨, ì²™ì¶”ê³¨ ë“±
        r"([ê°€-í£]{1,5}ì¥ì• )\b" # ë°œë‹¬ì¥ì• , ì‹ì´ì¥ì•  ë“±
    ]
    
    for pattern in medical_patterns:
        for match in re.finditer(pattern, text):
            term = match.group(1)
            if term not in medical_terms and len(term) >= 2:
                medical_terms.append(term)
    
    return medical_terms

# 13) ì „ì²˜ë¦¬ + í† í°í™”
records = []
for _, row in df_raw.iterrows():
    definition = clean_text(row["definition"])
    symptoms = clean_text(row["symptoms"])
    therapy = clean_text(row["therapy"])
    combined = f"{definition} {symptoms} {therapy}"

    # 13.1) í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ í† í° ë¨¼ì € ì¶”ì¶œ
    whitelist_tokens = []
    for token in WHITELIST:
        if token in combined and token not in whitelist_tokens:
            whitelist_tokens.append(token)
    
    # 13.2) íŠ¹ìˆ˜ íŒ¨í„´ í† í° ì¶”ì¶œ
    special_tokens = extract_special_patterns(combined)
    
    # 13.3) ì˜í•™ ìš©ì–´ ì§ì ‘ ì¶”ì¶œ
    medical_tokens = extract_medical_terms(combined)
    
    # 13.4) í˜•íƒœì†Œ ë¶„ì„ì„ ìœ„í•œ í…ìŠ¤íŠ¸ì—ì„œ ì´ë¯¸ ì¶”ì¶œí•œ í† í° ì œê±°
    temp = combined
    for w in whitelist_tokens + special_tokens + medical_tokens:
        # ë‹¨ì–´ ê²½ê³„ë¡œ ì œê±° (ì •ê·œì‹ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬)
        temp = re.sub(rf"\b{re.escape(w)}\b", " ", temp)
    temp = re.sub(r"\s+", " ", temp).strip()

    # 13.5) ì˜¤ì§ ëª…ì‚¬ë§Œ ì¶”ì¶œ
    nouns = okt.nouns(temp)
    
    # 13.6) ìµœì¢… í† í° ìˆœì„œëŒ€ë¡œ ì¤‘ë³µ ì—†ì´ í•©ì¹˜ê¸°
    all_tokens = whitelist_tokens + special_tokens + medical_tokens + nouns
    final_tokens = []
    
    for token in all_tokens:
        if token and token not in final_tokens and is_valid_token(token):
            final_tokens.append(token)

    records.append({
        "disnm_ko":   row["disnm_ko"],
        "disnm_en":   row["disnm_en"],
        "dep":        row["dep"],
        "definition": definition,
        "symptoms":   row["symptoms"],
        "therapy":    row["therapy"],
        "tokens":     final_tokens,
        "doc":        " ".join(final_tokens)
    })

# 14) ì¤‘ë³µ(doc) ì œê±°
df_proc = pd.DataFrame(records).drop_duplicates(subset=["disnm_ko", "doc"])

# 15) ê²°ê³¼ë¥¼ testdis í…Œì´ë¸”ë¡œ ì ì¬
df_proc.to_sql(
    "testdis",
    engine,
    if_exists="replace",
    index=False,
    dtype={
        "disnm_ko":   TEXT,
        "disnm_en":   TEXT,
        "dep":        TEXT,
        "definition": TEXT,
        "symptoms":   TEXT,
        "therapy":    TEXT,
        "tokens":     JSONB,
        "doc":        TEXT
    }
)

print("âœ… testdis í…Œì´ë¸”ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
print(f"ğŸ”¢ ì´ {len(df_proc)}ê°œì˜ ì§ˆë³‘ ë°ì´í„°ê°€ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")