# ~/code/backend/app/api/disease_api.py
"""
ì§ˆë³‘ ì¶”ì²œ API - ë§¤ì¹­ ë¡œì§ ê°œì„  ë²„ì „ (ë²¡í„° ë¬¸ì œ í•´ê²° ì „ê¹Œì§€ ì‚¬ìš©)
"""

from fastapi import APIRouter, HTTPException, Depends
from sqlalchemy.orm import Session
from sqlalchemy import text
from typing import List, Dict, Any, Optional
import logging
import numpy as np
from pydantic import BaseModel
import json
import re
from konlpy.tag import Okt
from sklearn.feature_extraction.text import TfidfVectorizer

from app.models.database import get_db

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

# API ë¼ìš°í„° ì´ˆê¸°í™”
router = APIRouter(prefix="/api", tags=["disease"])

# ì „ì—­ ë³€ìˆ˜ (ë©”ëª¨ë¦¬ ìºì‹œìš©)
_medical_mappings = {}
_tfidf_vectorizer = None
_tfidf_vocabulary = {}
_tfidf_idf_weights = {}
_disease_vectors_cache = []
_is_initialized = False

# Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
okt = Okt()

# ìš”ì²­/ì‘ë‹µ ëª¨ë¸
class DiseaseRecommendRequest(BaseModel):
    original_text: str
    positive: List[str] = []
    negative: List[str] = []

class DiseaseRecommendation(BaseModel):
    disease_id: str
    disease_name_ko: str
    department: Optional[str] = None
    similarity_score: float
    final_score: float
    matched_tokens: List[str] = []

class DiseaseRecommendResponse(BaseModel):
    recommendations: List[DiseaseRecommendation]
    debug_info: Dict[str, Any]

def safe_json_loads(data):
    """PostgreSQL JSONB ë°ì´í„°ë¥¼ ì•ˆì „í•˜ê²Œ ë¡œë“œ"""
    if isinstance(data, str):
        return json.loads(data)
    elif isinstance(data, dict):
        return data
    else:
        return dict(data) if hasattr(data, '__iter__') else data

def extract_morphemes(text: str) -> List[str]:
    """í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬ í˜•íƒœì†Œ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
    try:
        if not text or not text.strip():
            return []
        
        # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        cleaned_text = re.sub(r'[^\w\s]', ' ', text)
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
        
        # í˜•íƒœì†Œ ë¶„ì„ìœ¼ë¡œ ëª…ì‚¬ ì¶”ì¶œ
        nouns = okt.nouns(cleaned_text)
        
        # ì˜í•™ ê´€ë ¨ í‚¤ì›Œë“œ ì§ì ‘ ì¶”ì¶œ (í˜•íƒœì†Œ ë¶„ì„ ë³´ì™„)
        medical_keywords = []
        medical_patterns = [
            r'ë¨¸ë¦¬.*?ì•„í”„', r'ë‘í†µ', r'ë¨¸ë¦¬',
            r'ì—´', r'ë°œì—´', r'ê³ ì—´',
            r'ê¸°ì¹¨', r'ê°€ë˜', r'ì½§ë¬¼',
            r'ë°°.*?ì•„í”„', r'ë³µí†µ', r'ë°°',
            r'ëª©.*?ì•„í”„', r'ì¸í›„í†µ', r'ëª©',
            r'ê°€ìŠ´.*?ì•„í”„', r'í‰í†µ', r'ê°€ìŠ´',
            r'ì–´ì§€ëŸ¬', r'í˜„ê¸°ì¦',
            r'êµ¬í† ', r'í† í•˜', r'ë©”ìŠ¤êº¼',
            r'ì„¤ì‚¬', r'ë³€ë¹„', r'ì†Œí™”ë¶ˆëŸ‰',
            r'í”¼ë¡œ', r'ë¬´ê¸°ë ¥', r'ê¶Œíƒœê°'
        ]
        
        for pattern in medical_patterns:
            if re.search(pattern, text):
                if 'ë¨¸ë¦¬' in pattern or 'ë‘í†µ' in pattern:
                    medical_keywords.append('ë‘í†µ')
                elif 'ì—´' in pattern or 'ë°œì—´' in pattern:
                    medical_keywords.append('ë°œì—´')
                elif 'ê¸°ì¹¨' in pattern:
                    medical_keywords.append('ê¸°ì¹¨')
                elif 'ë³µí†µ' in pattern or 'ë°°' in pattern:
                    medical_keywords.append('ë³µí†µ')
                elif 'ëª©' in pattern or 'ì¸í›„í†µ' in pattern:
                    medical_keywords.append('ì¸í›„í†µ')
                elif 'ê°€ìŠ´' in pattern or 'í‰í†µ' in pattern:
                    medical_keywords.append('í‰í†µ')
                elif 'ì–´ì§€ëŸ¬' in pattern or 'í˜„ê¸°ì¦' in pattern:
                    medical_keywords.append('í˜„ê¸°ì¦')
                elif 'êµ¬í† ' in pattern or 'í† í•˜' in pattern or 'ë©”ìŠ¤êº¼' in pattern:
                    medical_keywords.append('êµ¬í† ')
                elif 'ì„¤ì‚¬' in pattern:
                    medical_keywords.append('ì„¤ì‚¬')
                elif 'í”¼ë¡œ' in pattern:
                    medical_keywords.append('í”¼ë¡œ')
        
        # ëª…ì‚¬ + ì˜í•™ í‚¤ì›Œë“œ ê²°í•©
        all_tokens = list(set(nouns + medical_keywords))
        
        # ê¸¸ì´ 1 ì´ìƒì¸ í† í°ë§Œ í•„í„°ë§
        filtered_tokens = [token for token in all_tokens if len(token) >= 1 and token.strip()]
        
        logger.debug(f"í˜•íƒœì†Œ ë¶„ì„: '{text}' -> ëª…ì‚¬={nouns}, ì˜í•™í‚¤ì›Œë“œ={medical_keywords}, ìµœì¢…={filtered_tokens}")
        return filtered_tokens
        
    except Exception as e:
        logger.error(f"í˜•íƒœì†Œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return []

def load_medical_mappings(db: Session) -> Dict[str, str]:
    """ì˜í•™ ìš©ì–´ ë§¤í•‘ ì‚¬ì „ ë¡œë“œ"""
    global _medical_mappings
    
    try:
        if _medical_mappings:  # ì´ë¯¸ ë¡œë“œë¨
            return _medical_mappings
        
        logger.info("ì˜í•™ ìš©ì–´ ë§¤í•‘ ì‚¬ì „ ë¡œë“œ ì‹œì‘")
        
        query = text("""
            SELECT common_term, medical_term 
            FROM medical_term_mappings 
            ORDER BY common_term
        """)
        
        results = db.execute(query).fetchall()
        
        for row in results:
            _medical_mappings[row.common_term.strip()] = row.medical_term.strip()
        
        logger.info(f"ë§¤í•‘ ì‚¬ì „ ë¡œë“œ ì™„ë£Œ: {len(_medical_mappings)}ê°œ")
        return _medical_mappings
        
    except Exception as e:
        logger.error(f"ë§¤í•‘ ì‚¬ì „ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return {}

def find_matching_expressions(user_tokens: List[str], 
                            mapping_dict: Dict[str, str],
                            min_overlap: int = 1) -> List[str]:
    """
    ì‚¬ìš©ì í† í°ê³¼ ì¼ìƒí‘œí˜„ ê°„ ê²¹ì¹˜ëŠ” í† í°ì´ ì„ê³„ê°’ ì´ìƒì¸ ê²½ìš° ì˜í•™ìš©ì–´ ë°˜í™˜ (ê°œì„ ëœ ë²„ì „)
    """
    try:
        matched_medical_terms = []
        
        logger.debug(f"ë§¤ì¹­ ì‹œë„ - ì‚¬ìš©ì í† í°: {user_tokens}")
        
        for common_expr, medical_term in mapping_dict.items():
            # ì¼ìƒí‘œí˜„ë„ í˜•íƒœì†Œ ë¶„ì„
            common_tokens = extract_morphemes(common_expr)
            
            # ì •í™•í•œ ë§¤ì¹­ ìš°ì„  í™•ì¸
            exact_match = False
            for user_token in user_tokens:
                for common_token in common_tokens:
                    # ì •í™•í•œ ë§¤ì¹­ (ê¸¸ì´ 2 ì´ìƒì¸ ê²½ìš°ë§Œ)
                    if len(user_token) >= 2 and len(common_token) >= 2 and user_token == common_token:
                        exact_match = True
                        break
                if exact_match:
                    break
            
            if exact_match:
                matched_medical_terms.append(medical_term)
                logger.debug(f"ì •í™• ë§¤ì¹­: '{common_expr}' -> '{medical_term}'")
                continue
            
            # ë¶€ë¶„ ë§¤ì¹­ (ë” ì—„ê²©í•œ ì¡°ê±´)
            meaningful_match = False
            for user_token in user_tokens:
                for common_token in common_tokens:
                    # ë¶€ë¶„ ë§¤ì¹­ ì¡°ê±´ì„ ë” ì—„ê²©í•˜ê²Œ
                    if (len(user_token) >= 3 and len(common_token) >= 3 and 
                        (user_token in common_token or common_token in user_token)):
                        # ë‹¨ìˆœíˆ "í†µ"ë§Œ ê²¹ì¹˜ëŠ” ê²ƒì€ ì œì™¸
                        if user_token not in ['í†µ', 'ì•„í”„', 'ì•„í””'] and common_token not in ['í†µ', 'ì•„í”„', 'ì•„í””']:
                            meaningful_match = True
                            break
                if meaningful_match:
                    break
            
            # ê²¹ì¹˜ëŠ” í† í° ìˆ˜ ê³„ì‚° (ê¸¸ì´ 2 ì´ìƒì¸ í† í°ë§Œ)
            valid_user_tokens = [t for t in user_tokens if len(t) >= 2]
            valid_common_tokens = [t for t in common_tokens if len(t) >= 2]
            overlap_tokens = set(valid_user_tokens) & set(valid_common_tokens)
            overlap_count = len(overlap_tokens)
            
            # ìµœì¢… ë§¤ì¹­ ì¡°ê±´: ì •í™•í•œ ë§¤ì¹­ ë˜ëŠ” ì˜ë¯¸ìˆëŠ” ë¶€ë¶„ë§¤ì¹­ ë˜ëŠ” 2ê°œ ì´ìƒ í† í° ê²¹ì¹¨
            if meaningful_match or overlap_count >= 2:
                matched_medical_terms.append(medical_term)
                logger.debug(f"ë¶€ë¶„ ë§¤ì¹­: '{common_expr}' -> '{medical_term}' (ê²¹ì¹¨: {overlap_count}, ì˜ë¯¸ë§¤ì¹­: {meaningful_match}, ê²¹ì¹œí† í°: {overlap_tokens})")
        
        # ì¤‘ë³µ ì œê±°
        unique_terms = list(set(matched_medical_terms))
        logger.debug(f"ìµœì¢… ë§¤ì¹­ëœ ì˜í•™ìš©ì–´: {unique_terms}")
        
        return unique_terms
        
    except Exception as e:
        logger.error(f"í‘œí˜„ ë§¤ì¹­ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return []

def load_tfidf_components(db: Session) -> bool:
    """TF-IDF ë²¡í„°í™”ì— í•„ìš”í•œ ì»´í¬ë„ŒíŠ¸ë“¤ ë¡œë“œ (ìˆ˜ì •ëœ ë²„ì „)"""
    global _tfidf_vectorizer, _tfidf_vocabulary, _tfidf_idf_weights
    
    try:
        if _tfidf_vectorizer is not None:  # ì´ë¯¸ ë¡œë“œë¨
            return True
        
        logger.info("TF-IDF ì»´í¬ë„ŒíŠ¸ ë¡œë“œ ì‹œì‘")
        
        # ë©”íƒ€ë°ì´í„° ì¡°íšŒ
        metadata_query = text("""
            SELECT 
                vocabulary,
                idf_weights,
                feature_count,
                min_df,
                max_df,
                max_features
            FROM tfidf_metadata 
            ORDER BY created_at DESC 
            LIMIT 1
        """)
        
        result = db.execute(metadata_query).fetchone()
        if not result:
            logger.error("TF-IDF ë©”íƒ€ë°ì´í„° ì—†ìŒ")
            return False
        
        logger.info(f"ë©”íƒ€ë°ì´í„° íƒ€ì… í™•ì¸: vocabulary={type(result.vocabulary)}, idf_weights={type(result.idf_weights)}")
        
        # JSONB ë°ì´í„° ì•ˆì „í•˜ê²Œ ë¡œë“œ
        _tfidf_vocabulary = safe_json_loads(result.vocabulary)
        _tfidf_idf_weights = safe_json_loads(result.idf_weights)
        
        logger.info(f"íŒŒì‹± ì™„ë£Œ: vocabulary_size={len(_tfidf_vocabulary)}, idf_weights_size={len(_tfidf_idf_weights)}")
        
        # ì–´íœ˜ì‚¬ì „ ìƒ˜í”Œ í™•ì¸ (ë””ë²„ê¹…ìš©)
        sample_vocab = dict(list(_tfidf_vocabulary.items())[:10])
        logger.info(f"ì–´íœ˜ì‚¬ì „ ìƒ˜í”Œ: {sample_vocab}")
        
        # í•œê¸€ í† í° ê°œìˆ˜ í™•ì¸
        korean_tokens = [word for word in _tfidf_vocabulary.keys() 
                        if any('\uAC00' <= c <= '\uD7A3' for c in word)]
        logger.info(f"í•œê¸€ í† í° ê°œìˆ˜: {len(korean_tokens)}")
        logger.info(f"í•œê¸€ í† í° ìƒ˜í”Œ: {korean_tokens[:10]}")
        
        # TfidfVectorizer ì¬êµ¬ì„±
        _tfidf_vectorizer = TfidfVectorizer(
            min_df=result.min_df or 2,
            max_df=result.max_df or 0.8,
            max_features=result.max_features,  # Noneì¼ ìˆ˜ ìˆìŒ
            ngram_range=(1, 1),  # 1-gramë§Œ ì‚¬ìš©
            token_pattern=None,
            tokenizer=lambda text: text.split(),
            lowercase=False,
            norm='l2',
            use_idf=True,
            smooth_idf=True,
            sublinear_tf=True
        )
        
        # ì–´íœ˜ì‚¬ì „ê³¼ IDF ê°€ì¤‘ì¹˜ ì„¤ì •
        _tfidf_vectorizer.vocabulary_ = _tfidf_vocabulary
        _tfidf_vectorizer.idf_ = np.array([
            _tfidf_idf_weights.get(word, 1.0) 
            for word in _tfidf_vocabulary.keys()
        ])
        
        logger.info(f"TF-IDF ì»´í¬ë„ŒíŠ¸ ë¡œë“œ ì™„ë£Œ: vocabulary_size={len(_tfidf_vocabulary)}")
        return True
        
    except Exception as e:
        logger.error(f"TF-IDF ì»´í¬ë„ŒíŠ¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        import traceback
        logger.error(f"ìƒì„¸ ì—ëŸ¬: {traceback.format_exc()}")
        return False

def create_fallback_vector(tokens: List[str]) -> np.ndarray:
    """vocabulary ë§¤ì¹­ì´ ì‹¤íŒ¨í•  ê²½ìš° ì‚¬ìš©í•˜ëŠ” fallback ë²¡í„°"""
    try:
        vector_size = len(_tfidf_vocabulary) if _tfidf_vocabulary else 1000
        vector = np.zeros(vector_size)
        
        # ì˜í•™ìš©ì–´ë³„ ê³ ì • ê°€ì¤‘ì¹˜
        medical_weights = {
            'ë‘í†µ': 0.8, 'í¸ë‘í†µ': 0.8, 'ë¨¸ë¦¬': 0.6,
            'ë°œì—´': 0.8, 'ì—´': 0.7, 'ê³ ì—´': 0.8,
            'ê¸°ì¹¨': 0.7, 'ê°€ë˜': 0.6,
            'ë³µí†µ': 0.7, 'ë°°': 0.5,
            'ì¸í›„í†µ': 0.7, 'ëª©': 0.5,
            'í˜„ê¸°ì¦': 0.7, 'ì–´ì§€ëŸ¬': 0.6,
            'êµ¬í† ': 0.7, 'ë©”ìŠ¤êº¼': 0.6,
            'ì„¤ì‚¬': 0.7, 'ë³€ë¹„': 0.6,
            'í”¼ë¡œ': 0.6, 'ë¬´ê¸°ë ¥': 0.6
        }
        
        # í† í°ë³„ ê°€ì¤‘ì¹˜ ì ìš©
        total_weight = 0
        for token in tokens:
            if token in medical_weights:
                weight = medical_weights[token]
                # í•´ì‹œ ê¸°ë°˜ìœ¼ë¡œ ë²¡í„° ìœ„ì¹˜ ê²°ì •
                idx = abs(hash(token)) % vector_size
                vector[idx] = weight
                total_weight += weight
                logger.debug(f"Fallback ë²¡í„°: '{token}' -> index {idx}, weight {weight}")
        
        # ì •ê·œí™”
        if total_weight > 0:
            vector = vector / np.linalg.norm(vector)
            logger.debug(f"Fallback ë²¡í„° ìƒì„±: non_zero={np.count_nonzero(vector)}")
        
        return vector
        
    except Exception as e:
        logger.error(f"Fallback ë²¡í„° ìƒì„± ì‹¤íŒ¨: {str(e)}")
        return np.zeros(len(_tfidf_vocabulary) if _tfidf_vocabulary else 1000)

def vectorize_tokens(tokens: List[str]) -> Optional[np.ndarray]:
    """í† í° ë¦¬ìŠ¤íŠ¸ë¥¼ TF-IDF ë²¡í„°ë¡œ ë³€í™˜ (ê°œì„ ëœ ë²„ì „)"""
    try:
        if not tokens or not _tfidf_vocabulary:
            logger.warning("í† í°ì´ ì—†ê±°ë‚˜ ì–´íœ˜ì‚¬ì „ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            return None
        
        logger.debug(f"ë²¡í„°í™” ì‹œë„ - ì…ë ¥ í† í°: {tokens}")
        
        # ì§ì ‘ ì–´íœ˜ì‚¬ì „ì—ì„œ í† í° ë§¤ì¹­ (ë” ì—„ê²©í•œ ë§¤ì¹­)
        valid_tokens = []
        matched_info = []
        
        for token in tokens:
            # 1. ì •í™•í•œ ë§¤ì¹­ (ìš°ì„ ìˆœìœ„)
            if token in _tfidf_vocabulary:
                valid_tokens.append(token)
                matched_info.append(f"ì •í™•ë§¤ì¹­: {token} -> {_tfidf_vocabulary[token]}")
            # 2. ê¸¸ì´ 3 ì´ìƒì¸ í† í°ë§Œ ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
            elif len(token) >= 3:
                found_match = False
                best_match = None
                max_overlap = 0
                
                for vocab_word in _tfidf_vocabulary.keys():
                    if len(vocab_word) >= 3:
                        # ë” ì—„ê²©í•œ ë¶€ë¶„ ë§¤ì¹­ ì¡°ê±´
                        if token in vocab_word:
                            overlap = len(token)
                            if overlap > max_overlap:
                                max_overlap = overlap
                                best_match = vocab_word
                                found_match = True
                        elif vocab_word in token and len(vocab_word) >= 3:
                            overlap = len(vocab_word)
                            if overlap > max_overlap:
                                max_overlap = overlap
                                best_match = vocab_word
                                found_match = True
                
                if found_match and best_match:
                    valid_tokens.append(best_match)
                    matched_info.append(f"ë¶€ë¶„ë§¤ì¹­: {token} -> {best_match} (ê²¹ì¹¨ê¸¸ì´: {max_overlap})")
                else:
                    matched_info.append(f"ë§¤ì¹­ì‹¤íŒ¨: {token}")
            else:
                matched_info.append(f"ê¸¸ì´ë¶€ì¡±: {token} (ê¸¸ì´: {len(token)})")
        
        logger.debug(f"ë§¤ì¹­ ê²°ê³¼: {matched_info}")
        
        if not valid_tokens:
            logger.warning(f"ìœ íš¨í•œ í† í° ì—†ìŒ. ì›ë³¸ í† í°: {tokens}")
            return create_fallback_vector(tokens)
        
        logger.debug(f"ìœ íš¨í•œ í† í°ë“¤: {valid_tokens}")
        
        # ìˆ˜ë™ TF-IDF ë²¡í„° ìƒì„±
        vector_size = len(_tfidf_vocabulary)
        vector = np.zeros(vector_size)
        
        # í† í° ë¹ˆë„ ê³„ì‚°
        token_counts = {}
        for token in valid_tokens:
            token_counts[token] = token_counts.get(token, 0) + 1
        
        total_tokens = len(valid_tokens)
        
        # TF-IDF ê³„ì‚°
        for token, count in token_counts.items():
            if token in _tfidf_vocabulary:
                idx = _tfidf_vocabulary[token]
                tf = count / total_tokens
                idf = _tfidf_idf_weights.get(token, 1.0)
                
                # Sublinear TF ì ìš© (scikit-learnê³¼ ë™ì¼)
                if tf > 0:
                    tf = 1 + np.log(tf)
                
                vector[idx] = tf * idf
                logger.debug(f"ë²¡í„° ì„¤ì •: {token} (idx={idx}) tf={tf:.3f} idf={idf:.3f} value={tf*idf:.3f}")
        
        # L2 ì •ê·œí™”
        norm = np.linalg.norm(vector)
        if norm > 0:
            vector = vector / norm
            logger.debug(f"ë²¡í„° ìƒì„± ì™„ë£Œ: non_zero={np.count_nonzero(vector)}, norm={norm:.4f}")
            return vector
        else:
            logger.warning("ë²¡í„° normì´ 0ì…ë‹ˆë‹¤. fallback ë²¡í„° ì‚¬ìš©")
            return create_fallback_vector(tokens)
        
    except Exception as e:
        logger.error(f"ë²¡í„°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
        import traceback
        logger.error(f"ìƒì„¸ ì—ëŸ¬: {traceback.format_exc()}")
        return create_fallback_vector(tokens)

def load_disease_vectors(db: Session, limit: int = 100) -> List[Dict[str, Any]]:
    """ì§ˆë³‘ ë²¡í„°ë“¤ì„ DBì—ì„œ ë¡œë“œ (ìˆ˜ì •ëœ ë²„ì „)"""
    global _disease_vectors_cache
    
    try:
        if _disease_vectors_cache:  # ìºì‹œëœ ë°ì´í„° ìˆìŒ
            return _disease_vectors_cache[:limit]
        
        logger.info("ì§ˆë³‘ ë²¡í„° ë¡œë“œ ì‹œì‘")
        
        query = text("""
            SELECT 
                disease_id,
                disease_name_ko,
                department,
                tfidf_vector,
                vector_norm
            FROM disease_vectors 
            WHERE vector_norm > 0 
            ORDER BY non_zero_count DESC
            LIMIT :limit
        """)
        
        results = db.execute(query, {"limit": limit * 2}).fetchall()  # ì—¬ìœ ë¶„ ë¡œë“œ
        
        for row in results:
            try:
                # JSONB í˜•íƒœì˜ sparse vectorë¥¼ dense vectorë¡œ ë³€í™˜
                sparse_vector = safe_json_loads(row.tfidf_vector)
                    
                dense_vector = np.zeros(len(_tfidf_vocabulary))
                
                for idx_str, value in sparse_vector.items():
                    idx = int(idx_str)
                    if 0 <= idx < len(_tfidf_vocabulary):
                        dense_vector[idx] = float(value)
                
                # ì •ê·œí™” í™•ì¸
                norm = np.linalg.norm(dense_vector)
                if norm > 0:
                    dense_vector = dense_vector / norm
                
                    _disease_vectors_cache.append({
                        'disease_id': row.disease_id,
                        'disease_name_ko': row.disease_name_ko,
                        'department': row.department,
                        'vector': dense_vector
                    })
                
            except Exception as vector_error:
                logger.warning(f"ë²¡í„° íŒŒì‹± ì‹¤íŒ¨ (ID: {row.disease_id}): {str(vector_error)}")
                continue
        
        logger.info(f"ì§ˆë³‘ ë²¡í„° ë¡œë“œ ì™„ë£Œ: {len(_disease_vectors_cache)}ê°œ")
        return _disease_vectors_cache[:limit]
        
    except Exception as e:
        logger.error(f"ì§ˆë³‘ ë²¡í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return []

def initialize_all_components(db: Session) -> bool:
    """ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
    global _is_initialized
    
    if _is_initialized:
        return True
    
    try:
        logger.info("ì „ì²´ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹œì‘")
        
        # 1. ì˜í•™ ìš©ì–´ ë§¤í•‘ ë¡œë“œ
        mappings = load_medical_mappings(db)
        if not mappings:
            logger.warning("ì˜í•™ ìš©ì–´ ë§¤í•‘ ë¡œë“œ ì‹¤íŒ¨")
        
        # 2. TF-IDF ì»´í¬ë„ŒíŠ¸ ë¡œë“œ
        if not load_tfidf_components(db):
            logger.error("TF-IDF ì»´í¬ë„ŒíŠ¸ ë¡œë“œ ì‹¤íŒ¨")
            return False
        
        # 3. ì§ˆë³‘ ë²¡í„° ë¡œë“œ
        disease_vectors = load_disease_vectors(db, limit=1000)
        if not disease_vectors:
            logger.error("ì§ˆë³‘ ë²¡í„° ë¡œë“œ ì‹¤íŒ¨")
            return False
        
        _is_initialized = True
        logger.info("ì „ì²´ ì´ˆê¸°í™” ì™„ë£Œ")
        return True
        
    except Exception as e:
        logger.error(f"ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        return False

@router.post("/disease", response_model=DiseaseRecommendResponse)
async def recommend_diseases(
    request: DiseaseRecommendRequest,
    db: Session = Depends(get_db)
):
    """ì§ˆë³‘ ì¶”ì²œ ë©”ì¸ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        logger.info(f"ì§ˆë³‘ ì¶”ì²œ ìš”ì²­: {request.original_text}")
        
        # 1. ì´ˆê¸°í™”
        if not initialize_all_components(db):
            raise HTTPException(status_code=500, detail="ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
        
        debug_info = {
            "step1_morphemes": {},
            "step2_mapping": {},
            "step3_vectorization": {},
            "step4_similarity": {}
        }
        
        # 2. ê¸ì •/ë¶€ì • ì„¸ê·¸ë¨¼íŠ¸ í˜•íƒœì†Œ ë¶„ì„
        positive_morphemes = []
        for segment in request.positive:
            morphemes = extract_morphemes(segment)
            positive_morphemes.extend(morphemes)
        
        negative_morphemes = []
        for segment in request.negative:
            morphemes = extract_morphemes(segment)
            negative_morphemes.extend(morphemes)
        
        debug_info["step1_morphemes"] = {
            "positive_segments": request.positive,
            "negative_segments": request.negative,
            "positive_morphemes": positive_morphemes,
            "negative_morphemes": negative_morphemes
        }
        
        # 3. ì¼ìƒí‘œí˜„ ë§¤ì¹­
        matched_medical_terms_pos = find_matching_expressions(
            positive_morphemes, _medical_mappings, min_overlap=1
        )
        matched_medical_terms_neg = find_matching_expressions(
            negative_morphemes, _medical_mappings, min_overlap=1
        )
        
        # 4. ìµœì¢… í† í° ì¡°í•© (ì˜í•™ìš©ì–´ + ë§¤ì¹­ë˜ì§€ ì•Šì€ í˜•íƒœì†Œ)
        final_positive_tokens = list(set(matched_medical_terms_pos + positive_morphemes))
        final_negative_tokens = list(set(matched_medical_terms_neg + negative_morphemes))
        
        debug_info["step2_mapping"] = {
            "matched_medical_pos": matched_medical_terms_pos,
            "matched_medical_neg": matched_medical_terms_neg,
            "final_positive_tokens": final_positive_tokens,
            "final_negative_tokens": final_negative_tokens
        }
        
        # 5. ì‹¤ì‹œê°„ ë²¡í„°í™”
        positive_vector = vectorize_tokens(final_positive_tokens)
        negative_vector = vectorize_tokens(final_negative_tokens)
        
        debug_info["step3_vectorization"] = {
            "positive_vector_nonzero": int(np.count_nonzero(positive_vector)) if positive_vector is not None else 0,
            "negative_vector_nonzero": int(np.count_nonzero(negative_vector)) if negative_vector is not None else 0,
            "vocabulary_size": len(_tfidf_vocabulary)
        }
        
        # 6. ì§ˆë³‘ ë²¡í„°ì™€ ìœ ì‚¬ë„ ê³„ì‚°
        disease_vectors = load_disease_vectors(db, limit=100)

        recommendations = []
        for i, disease in enumerate(disease_vectors):
            # ê¸ì • ìœ ì‚¬ë„
            pos_similarity = 0.0
            if positive_vector is not None:
                pos_similarity = abs(float(np.dot(positive_vector, disease['vector'])))  # abs() ì¶”ê°€
                
                # ë””ë²„ê¹…: í¸ë‘í†µ ì§ˆë³‘ë§Œ ìƒì„¸ ë¡œê·¸
                if 'í¸ë‘í†µ' in disease['disease_name_ko']:
                    logger.info(f"ğŸ” í¸ë‘í†µ ë””ë²„ê¹…:")
                    logger.info(f"  ì§ˆë³‘ëª…: {disease['disease_name_ko']}")
                    logger.info(f"  ì‚¬ìš©ì ë²¡í„° shape: {positive_vector.shape}")
                    logger.info(f"  ì§ˆë³‘ ë²¡í„° shape: {disease['vector'].shape}")
                    logger.info(f"  ì‚¬ìš©ì ë²¡í„° nonzero: {np.count_nonzero(positive_vector)}")
                    logger.info(f"  ì§ˆë³‘ ë²¡í„° nonzero: {np.count_nonzero(disease['vector'])}")
                    logger.info(f"  ë‚´ì  ê²°ê³¼: {pos_similarity}")
                    logger.info(f"  ì‚¬ìš©ì ë²¡í„° norm: {np.linalg.norm(positive_vector)}")
                    logger.info(f"  ì§ˆë³‘ ë²¡í„° norm: {np.linalg.norm(disease['vector'])}")
            
            # ë¶€ì • ìœ ì‚¬ë„
            neg_similarity = 0.0
            if negative_vector is not None:
                neg_similarity = float(np.dot(negative_vector, disease['vector']))
            
            # ìµœì¢… ì ìˆ˜ (ê¸ì • - ë¶€ì •*0.3) - ë¶€ì • ê°€ì¤‘ì¹˜ ë‚®ì¶¤
            final_score = pos_similarity - (neg_similarity * 0.3)
            
            # ìµœì†Œ ì ìˆ˜ ì„ê³„ê°’ ë‚®ì¶¤ (0.001 ì´ìƒ)
            if final_score > 0.001:
                recommendations.append(DiseaseRecommendation(
                    disease_id=disease['disease_id'],
                    disease_name_ko=disease['disease_name_ko'],
                    department=disease.get('department'),
                    similarity_score=pos_similarity,
                    final_score=final_score,
                    matched_tokens=final_positive_tokens
                ))
        
        # 7. ì •ë ¬ ë° ìƒìœ„ 10ê°œë§Œ ë°˜í™˜
        recommendations.sort(key=lambda x: x.final_score, reverse=True)
        recommendations = recommendations[:10]
        
        debug_info["step4_similarity"] = {
            "total_candidates": len(disease_vectors),
            "above_zero": len(recommendations),
            "top_3_scores": [r.final_score for r in recommendations[:3]],
            "positive_vector_norm": float(np.linalg.norm(positive_vector)) if positive_vector is not None else 0,
            "sample_disease_vector_norms": [float(np.linalg.norm(d['vector'])) for d in disease_vectors[:3]]
        }
        
        logger.info(f"ì¶”ì²œ ì™„ë£Œ: {len(recommendations)}ê°œ ê²°ê³¼")
        
        return DiseaseRecommendResponse(
            recommendations=recommendations,
            debug_info=debug_info
        )
        
    except Exception as e:
        logger.error(f"ì§ˆë³‘ ì¶”ì²œ ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/status")
async def get_system_status(db: Session = Depends(get_db)):
    """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
    try:
        return {
            "is_initialized": _is_initialized,
            "medical_mappings_count": len(_medical_mappings),
            "vocabulary_size": len(_tfidf_vocabulary),
            "disease_vectors_count": len(_disease_vectors_cache),
            "tfidf_ready": _tfidf_vectorizer is not None
        }
        
    except Exception as e:
        return {"error": str(e)}

@router.post("/initialize")
async def manual_initialize(db: Session = Depends(get_db)):
    """ìˆ˜ë™ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    try:
        global _is_initialized, _medical_mappings, _tfidf_vocabulary, _tfidf_idf_weights, _disease_vectors_cache
        
        # ê¸°ì¡´ ìºì‹œ ì´ˆê¸°í™”
        _medical_mappings.clear()
        _tfidf_vocabulary.clear()
        _tfidf_idf_weights.clear()
        _disease_vectors_cache.clear()
        _is_initialized = False
        
        # ê°•ì œ ì´ˆê¸°í™”
        success = initialize_all_components(db)
        
        return {
            "initialization_success": success,
            "is_initialized": _is_initialized,
            "medical_mappings_count": len(_medical_mappings),
            "vocabulary_size": len(_tfidf_vocabulary),
            "disease_vectors_count": len(_disease_vectors_cache),
            "tfidf_ready": _tfidf_vectorizer is not None
        }
        
    except Exception as e:
        logger.error(f"ìˆ˜ë™ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return {"error": str(e)}

@router.get("/test-db")
async def test_database_connection(db: Session = Depends(get_db)):
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë° í…Œì´ë¸” í™•ì¸"""
    try:
        # ë§¤í•‘ í…Œì´ë¸” í™•ì¸
        mapping_query = text("SELECT COUNT(*) FROM medical_term_mappings")
        mapping_count = db.execute(mapping_query).scalar()
        
        # TF-IDF ë©”íƒ€ë°ì´í„° í™•ì¸
        metadata_query = text("SELECT COUNT(*) FROM tfidf_metadata")
        metadata_count = db.execute(metadata_query).scalar()
        
        # ì§ˆë³‘ ë²¡í„° í™•ì¸
        vector_query = text("SELECT COUNT(*) FROM disease_vectors")
        vector_count = db.execute(vector_query).scalar()
        
        # ìƒ˜í”Œ ë©”íƒ€ë°ì´í„° ì¡°íšŒ
        sample_metadata_query = text("""
            SELECT 
                id,
                feature_count,
                min_df,
                max_df,
                max_features,
                pg_typeof(vocabulary) as vocab_type,
                pg_typeof(idf_weights) as idf_type
            FROM tfidf_metadata 
            ORDER BY created_at DESC 
            LIMIT 1
        """)
        sample_metadata = db.execute(sample_metadata_query).fetchone()
        
        # ì–´íœ˜ì‚¬ì „ì—ì„œ ì§ì ‘ í•œê¸€ í† í° í™•ì¸
        vocab_check_query = text("""
            SELECT 
                COUNT(*) as total_keys
            FROM (
                SELECT jsonb_object_keys(vocabulary) as word
                FROM tfidf_metadata 
                ORDER BY created_at DESC 
                LIMIT 1
            ) t
        """)
        vocab_count = db.execute(vocab_check_query).scalar()
        
        return {
            "database_connection": "OK",
            "table_counts": {
                "medical_mappings": int(mapping_count),
                "tfidf_metadata": int(metadata_count),
                "disease_vectors": int(vector_count),
                "vocabulary_keys": int(vocab_count) if vocab_count else 0
            },
            "sample_metadata": {
                "id": sample_metadata.id if sample_metadata else None,
                "feature_count": sample_metadata.feature_count if sample_metadata else None,
                "vocabulary_type": sample_metadata.vocab_type if sample_metadata else None,
                "idf_weights_type": sample_metadata.idf_type if sample_metadata else None
            } if sample_metadata else None
        }
        
    except Exception as e:
        logger.error(f"DB í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return {"error": str(e)}

@router.get("/vocabulary-debug")
async def debug_vocabulary(db: Session = Depends(get_db)):
    """ì–´íœ˜ì‚¬ì „ ë””ë²„ê¹… ì—”ë“œí¬ì¸íŠ¸"""
    try:
        # ì´ˆê¸°í™” í™•ì¸
        if not _is_initialized:
            initialize_all_components(db)
        
        # ì–´íœ˜ì‚¬ì „ ë¶„ì„
        total_size = len(_tfidf_vocabulary)
        korean_words = []
        english_words = []
        medical_terms = []
        
        for word, idx in list(_tfidf_vocabulary.items())[:100]:  # ì²˜ìŒ 100ê°œë§Œ
            if any('\uAC00' <= c <= '\uD7A3' for c in word):
                korean_words.append((word, idx))
            elif word.isalpha() and all(ord(c) < 256 for c in word):
                english_words.append((word, idx))
            
            # ì˜í•™ìš©ì–´ í™•ì¸
            if any(keyword in word for keyword in ['í†µ', 'ì—´', 'ê¸°ì¹¨', 'ë³µí†µ', 'êµ¬í† ', 'ì„¤ì‚¬']):
                medical_terms.append((word, idx))
        
        # íŠ¹ì • ì˜í•™ìš©ì–´ ê²€ìƒ‰
        target_words = ['ë‘í†µ', 'ë³µí†µ', 'ì„¤ì‚¬', 'ê¸°ì¹¨', 'ë°œì—´', 'ì—´', 'êµ¬í† ']
        found_targets = {}
        for target in target_words:
            found_targets[target] = []
            for word, idx in _tfidf_vocabulary.items():
                if target in word or word == target:
                    found_targets[target].append((word, idx))
        
        return {
            "vocabulary_stats": {
                "total_size": total_size,
                "korean_sample": korean_words[:20],
                "english_sample": english_words[:20],
                "medical_terms": medical_terms[:20]
            },
            "target_search": found_targets,
            "idf_weights_sample": {word: _tfidf_idf_weights.get(word, 0) for word, _ in medical_terms[:10]}
        }
        
    except Exception as e:
        logger.error(f"ì–´íœ˜ì‚¬ì „ ë””ë²„ê¹… ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return {"error": str(e)}

@router.post("/test-vectorization")
async def test_vectorization(
    tokens: List[str],
    db: Session = Depends(get_db)
):
    """í† í° ë²¡í„°í™” í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        # ì´ˆê¸°í™” í™•ì¸
        if not _is_initialized:
            initialize_all_components(db)
        
        # ë²¡í„°í™” í…ŒìŠ¤íŠ¸
        vector = vectorize_tokens(tokens)
        
        # ê²°ê³¼ ë¶„ì„
        result = {
            "input_tokens": tokens,
            "vector_created": vector is not None,
            "vector_norm": float(np.linalg.norm(vector)) if vector is not None else 0,
            "non_zero_count": int(np.count_nonzero(vector)) if vector is not None else 0,
            "vocabulary_size": len(_tfidf_vocabulary)
        }
        
        # í† í°ë³„ ë§¤ì¹­ ì •ë³´
        token_matching = {}
        for token in tokens:
            if token in _tfidf_vocabulary:
                token_matching[token] = {
                    "status": "exact_match",
                    "index": _tfidf_vocabulary[token],
                    "idf_weight": _tfidf_idf_weights.get(token, 0)
                }
            else:
                # ë¶€ë¶„ ë§¤ì¹­ í™•ì¸
                partial_matches = []
                for vocab_word in list(_tfidf_vocabulary.keys())[:1000]:  # ì²˜ìŒ 1000ê°œë§Œ í™•ì¸
                    if token in vocab_word or vocab_word in token:
                        partial_matches.append(vocab_word)
                        if len(partial_matches) >= 5:  # ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ
                            break
                
                token_matching[token] = {
                    "status": "partial_match" if partial_matches else "no_match",
                    "partial_matches": partial_matches
                }
        
        result["token_matching"] = token_matching
        
        return result
        
    except Exception as e:
        logger.error(f"ë²¡í„°í™” í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return {"error": str(e)}

@router.get("/version-check")
async def check_version():
    """í˜„ì¬ ì½”ë“œ ë²„ì „ í™•ì¸"""
    return {
        "version": "best_working_v1.0",
        "timestamp": "2025-05-22-21:25",
        "features": [
            "improved_matching_logic",
            "strict_partial_matching", 
            "medical_term_prioritization",
            "fallback_vector_support",
            "comprehensive_debugging"
        ]
    }