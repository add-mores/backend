
"""
ì¦ìƒ ì¤‘ì‹¬ ê³ ìˆœë„ ì˜í•™ í† í°í™” ëª¨ë“ˆ
- ì¦ìƒ(sym) ì»¬ëŸ¼ë§Œ ì‚¬ìš©
- ë³µí•©ëª…ì‚¬ + ì˜í•™ì‚¬ì „ ìµœëŒ€ í™œìš©
- ëª…ì‚¬ ì¤‘ì‹¬ í† í°í™”
- ë¶€ì •ë¬¸ ì²˜ë¦¬ ì œì™¸ (APIì—ì„œ ì²˜ë¦¬ë¨)
"""

import os
import re
import pandas as pd
import json
import numpy as np
from konlpy.tag import Okt
import logging
from datetime import datetime
from typing import List, Dict, Set, Tuple
from collections import Counter

# íŒŒì¼ ê²½ë¡œ ì„¤ì •
DISEASE_CSV_PATH = "/Users/jacob/Desktop/token/disease_data.csv"
MED_TERMS_CSV_PATH = "/Users/jacob/Desktop/token/medical_terms_cleaned.csv"
OUTPUT_CSV_PATH = "/Users/jacob/Desktop/token/processed_disease_data_v2.csv"

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
LOG_DIR = "/Users/jacob/Desktop/token/logs"
os.makedirs(LOG_DIR, exist_ok=True)

# ë¡œê¹… ì„¤ì •
log_filename = os.path.join(LOG_DIR, f"tokenization_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_filename, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ë¶ˆìš©ì–´ ëª©ë¡ (ê¸°ì¡´ ì¬ì‚¬ìš© + ì¦ìƒ íŠ¹í™”)
STOPWORDS = [
    "ê°€ëŠ¥ì„±", "ê°€ì¥", "ê°€ì§€", "ê°ì†Œ", "ê°ì—¼", "ê°‘ìê¸°", "ê°œì›”", "ê±°ë‚˜", "ê±°ì˜",
    "ê²€ì‚¬", "ê²°ê³¼", "ê²°ì •", "ê²°í•", "ê²½ê³¼", "ê²½í–¥", "ê²½í—˜", "ê³ ë ¤", "ê³µê°„",
    "ê³µê¸‰", "ê³¼ë„", "ê³¼ì •", "ê´€ë ¨", "ê´€ì ˆ", "ê´€ì°°", "êµì •", "êµ¬ë¶„", "êµ¬ì¡°",
    "êµ­ì†Œ", "ê¸‰ì„±", "ê¸°ê°„", "ê¸°ëŠ”", "ê¸°ëŠ¥", "ê¸°ë©´", "ë‚˜ì´",
    "ë‚´ë¶€", "ë…¸ì¶œ", "ëŠ¥ë ¥", "ë‹¤ë¥¸", "ë‹¤ì‹œ", "ë‹¤ìŒ", "ë‹¬ë¦¬", "ëŒ€ê°œ", "ëŒ€ë¶€ë¶„", "ëŒ€í‘œ",
    "ëŒ€í•œ", "ëŒ€í•´", "ë„ì›€", "ë™ë°˜", "ë™ì•ˆ", "ë”°ë¼ì„œ", "ë•Œë¬¸", "ë˜í•œ", "ë§ˆë¹„",
    "ë§Œì„±", "ë§¤ìš°", "ë©´ì—­", "ëª¨ë‘", "ëª¨ë“ ", "ëª¨ì–‘", "ëª©ì ", "ë¬¸ì œ", "ë¬¼ì§ˆ",
    "ë°˜ë³µ", "ë°˜ì‘", "ë°œê²¬", "ë°œë‹¬", "ë°œë³‘", "ë°œìƒ", "ë°œì„±",
    "ë°©ë²•", "ë°©ì‚¬ì„ ", "ë²”ìœ„", "ë³€ì´", "ë³€í˜•", "ë³€í™”", "ë³´ê³ ", "ë³´ì´", "ë³´ì¡´", "ë³´í†µ",
    "ë³µìš©", "ë¶€ë¶„", "ë¶€ìœ„", "ë¶€ì¡±", "ë¶€ì¢…", "ë¶„ë¥˜",
    "ë¹„êµ", "ë¹„ì •ìƒ", "ë¹ˆë„", "ì‚¬ëŒ", "ì‚¬ë§", "ì‚¬ìš©", "ì‚¬ì´", "ì‚½ì…", "ìƒìŠ¹",
    "ìƒíƒœ", "ìƒí™©", "ìƒ‰ì†Œ", "ì„œì„œíˆ", "ì„ íƒ", "ì„±ì¸", "ì„±ì¥",
    "ì†Œê²¬", "ì†Œì‹¤", "ì†Œì•„", "ì†ìƒ", "ìˆ˜ë„", "ìˆ˜ë©´", "ìˆ˜ìˆ ", "ìˆ˜ì˜", "ìˆ˜ì¶•",
    "ì‹œê°„", "ì‹œê¸°", "ì‹œë„", "ì‹œìˆ ", "ì‹œì‘", "ì‹œí–‰", "ì‹ ì¥", "ì‹ ì²´",
    "ì•„ë˜", "ì•…í™”", "ì•ˆì •", "ì••ë°•", "ì•½ë¬¼", "ì•½ì œ", "ì–‘ìƒ", "ì–‘ì„±", "ì—¬ëŸ¬",
    "ì—¬ë¶€", "ì—¬ì„±", "ì—­í• ", "ì—°ë ¹", "ì—¼ìƒ‰", "ì—¼ì¦", "ì˜í–¥", "ì˜ˆë°©", "ì™„í™”", "ì™¸ë¶€",
    "ìš”ë²•", "ìš©ì–´", "ìš°ë¦¬", "ìš°ë¦¬ë‚˜ë¼", "ìš°ì„ ", "ìš´ë™", "ì›€ì§ì„", "ì›ì¸", "ìœ„ì¶•", "ìœ„ì¹˜",
    "ìœ„í•´", "ìœ„í—˜", "ìœ ë°œ", "ìœ ì§€", "ì˜ë¯¸", "ì˜ì‚¬", "ì˜ì‹", "ì˜ì‹¬", "ì´ë‚´", "ì´ë¡œ","êµ¬ê¸‰ì°¨",
    "ì´ë£¨", "ì´ìƒ", "ì´ì‹", "ì´ì™¸", "ì´ìš©", "ì´ì „", "ì´í•˜", "ì´í›„", "ì¼ë°˜", "ì¼ë¶€","í•­ìƒ","íƒ€ê³ ",
    "ì¼ì‹œ", "ì„ìƒ", "ìê·¹", "ìì—°", "ìì£¼", "ìì²´", "ì‘ìš©", "ì¥ê¸°", "ì¥ì• ", "ì¬ë°œ","ì˜¤íˆë ¤",
    "ì €í•˜", "ì „ì‹ ", "ì „ì²´", "ì ˆê°œ", "ì ˆì œ", "ì ì°¨", "ì •ë„", "ì •ìƒ","íŠ¹ì´","ê²‰ë³´ê¸°","ë°˜ë“œì‹œ",
    "ì •ì˜", "ì œê±°", "ì œí•œ", "ì¡°ê¸°", "ì¡°ì ˆ", "ì¡´ì¬", "ì¢…ë¥˜","ì´¬ì˜","ë³´ì•„","ì§„ì „","ë°˜ë©´","ì €ì ˆë¡œ",
    "ì£¼ë¡œ", "ì£¼ë³€", "ì£¼ì‚¬", "ì£¼ìš”", "ì£¼ìœ„", "ì¤‘ì´", "ì¦ê°€", "ì¦ìƒ", "ì¦ì‹","ì „ë‹¬",
    "ì§€ì†", "ì§€ì—°", "ì§ì ‘", "ì§„ë‹¨", "ì§„í–‰", "ì§ˆë³‘", "ì§ˆí™˜", "ì°¨ì´", "ì°¨ì§€", "ì²˜ìŒ",
    "ì²´ë‚´", "ì´ˆê¸°", "ì´ˆë˜", "ìµœê·¼", "ì¶œìƒ", "ì¹˜ë£Œ", "ì¹˜ë£Œë²•", "ì¹¨ë²”","ì™¸êµ­","ì¡°ì‚¬",
    "í¬ê²Œ", "í¬ê¸°", "í†µì¦", "í†µí•´", "íˆ¬ì—¬", "íŠ¹ì§•", "í¬í•¨", "í‘œë©´", "í”¼ë¡œ","ì¸êµ¬","ìƒëŒ€",
    "í•„ìš”", "í•˜ë‚˜", "í•­ìƒì œ", "í•´ë‹¹", "í–‰ë™", "í˜„ìƒ", "í˜„ì¬","ì •ê¸°","ì£¼ê´€","ë°”ë¡œ",
    "í˜•ì„±", "í˜•íƒœ", "í˜¸ë¥´ëª¬", "í˜¸ì†Œ", "í˜¸ì „", "í™•ì¥", "í™˜ì", "í™œë™", "íšŒë³µ","ìˆ˜ë…„",
    "íš¨ê³¼","í•˜ì™€"
    # ì¼ë°˜ í•œêµ­ì–´ ë¶ˆìš©ì–´
    "ìˆë‹¤", "ì—†ë‹¤", "ë˜ë‹¤", "í•˜ë‹¤", "ì´ë‹¤", "ê°™ë‹¤", "ë•Œë¬¸", "ë”°ë¼ì„œ",
    "ê·¸ëŸ¬ë‚˜", "í•˜ì§€ë§Œ", "ë˜í•œ", "ë“±", "ë°", "ì—ì„œ", "ìœ¼ë¡œ", "ì´ë‚˜","ë•Œë¡œëŠ”",
    "ê·¸ë¦¬ê³ ", "ë˜ëŠ”", "ê²½ìš°", "í†µí•´", "ìœ„í•´", "ëŒ€í•´", "ì´ëŸ°", "ê·¸ëŸ°","ê°€ìš´ë°","í•œí¸"
    # í•œêµ­ì–´ ì¡°ì‚¬/ì ‘ì†ì‚¬/ì–´ë¯¸
    "ì˜", "ì—", "ì„", "ë¥¼", "ì€", "ëŠ”", "ì´", "ê°€", "ì™€", "ê³¼", "ë¡œ", "ìœ¼ë¡œ", 
    "ì—ì„œ", "ë¶€í„°", "ê¹Œì§€", "ì²˜ëŸ¼", "ë§Œí¼", "ë³´ë‹¤", "ê°™ì´", "ê°™ì€", "ì´ë‚˜", 
    "ê±°ë‚˜", "ë”", "ì¢€", "ì˜", "ëª»",
    # ì¦ìƒ ê´€ë ¨ ì¼ë°˜ ìš©ì–´ (ì˜ë¯¸ í¬ì„ ë°©ì§€)
    "ì¦ìƒ", "ì§ˆë³‘", "ì§ˆí™˜", "í™˜ì", "ì¹˜ë£Œ", "ìƒíƒœ", "ê²½ìš°", "ì •ë„", "ë¶€ë¶„"
]

# ë³µí•©ëª…ì‚¬ ëª©ë¡ (ê¸°ì¡´ ì¬ì‚¬ìš©)
COMPOUND_NOUNS = [
    "ê¸‰ì„± ìœ„ì—¼", "ë§Œì„± ìœ„ì—¼", "ê¸‰ì„± íë ´", "ë§Œì„± íë ´", "ê¸‰ì„± ê¸°ê´€ì§€ì—¼", "ë§Œì„± ê¸°ê´€ì§€ì—¼",
    "ê¸‰ì„± ì¶©ìˆ˜ì—¼", "ë§Œì„± ì¶©ìˆ˜ì—¼", "ê¸‰ì„± ì‹ ìš°ì‹ ì—¼", "ë§Œì„± ì‹ ìš°ì‹ ì—¼", "ê¸‰ì„± ê°„ì—¼", "ë§Œì„± ê°„ì—¼",
    "ê¸‰ì„± ì·Œì¥ì—¼", "ë§Œì„± ì·Œì¥ì—¼", "ê¸‰ì„± ë‹´ë‚­ì—¼", "ë‹´ë‚­ ê²°ì„", "ë‹´ì„ì¦", "ì‹ ì¥ ê²°ì„",
    "ìš”ê´€ ê²°ì„", "ë°©ê´‘ ê²°ì„", "ìš”ë¡œ ê²°ì„", "ì•Œë ˆë¥´ê¸°ì„± ë¹„ì—¼", "ì•Œë ˆë¥´ê¸° ë¹„ì—¼",
    "ì•Œë ˆë¥´ê¸° ê²°ë§‰ì—¼", "ì•Œë ˆë¥´ê¸° í”¼ë¶€ì—¼", "ì•„í† í”¼ í”¼ë¶€ì—¼", "ì ‘ì´‰ì„± í”¼ë¶€ì—¼",
    "ë‹¹ë‡¨ë³‘", "ì œ1í˜• ë‹¹ë‡¨ë³‘", "ì œ2í˜• ë‹¹ë‡¨ë³‘", "ì¸ìŠë¦° ì˜ì¡´ì„± ë‹¹ë‡¨ë³‘", "ì¸ìŠë¦° ë¹„ì˜ì¡´ì„± ë‹¹ë‡¨ë³‘",
    "ë‹¹ë‡¨ë³‘ì„± ì¼€í†¤ì‚°ì¦", "ë‹¹ë‡¨ë³‘ì„± ì‹ ì¦", "ë‹¹ë‡¨ë³‘ì„± ë§ë§‰ë³‘ì¦", "ë‹¹ë‡¨ë³‘ì„± ì‹ ê²½ë³‘ì¦",
    "ê³ í˜ˆì••", "ë³¸íƒœì„± ê³ í˜ˆì••", "ì´ì°¨ì„± ê³ í˜ˆì••", "í ê³ í˜ˆì••", "ë¬¸ë§¥ ê³ í˜ˆì••",
    "ê´€ìƒë™ë§¥ ì§ˆí™˜", "ê´€ìƒë™ë§¥ ê²½í™”ì¦", "ê´€ìƒë™ë§¥ í˜‘ì°©ì¦", "ì‹¬ê·¼ ê²½ìƒ‰", "ì‹¬ê·¼ í—ˆí˜ˆ",
    "ì‹¬ì¥ íŒë§‰ ì§ˆí™˜", "ì‹¬ë°© ì„¸ë™", "ì‹¬ì‹¤ ì„¸ë™", "ì‹¬ë¶€ì „", "ìš¸í˜ˆì„± ì‹¬ë¶€ì „",
    "ë‡Œì¡¸ì¤‘", "í—ˆí˜ˆì„± ë‡Œì¡¸ì¤‘", "ì¶œí˜ˆì„± ë‡Œì¡¸ì¤‘", "ì¼ê³¼ì„± í—ˆí˜ˆ ë°œì‘", "ë‡Œì¶œí˜ˆ",
    "ì§€ì£¼ë§‰í•˜ ì¶œí˜ˆ", "ê²½ë§‰í•˜ ì¶œí˜ˆ", "ê²½ë§‰ì™¸ ì¶œí˜ˆ", "ë‡Œë‚´ ì¶œí˜ˆ", "ë‡Œí˜ˆê´€ ì§ˆí™˜",
    "ìœ„ì‹ë„ ì—­ë¥˜ ì§ˆí™˜", "ìœ„ì‹ë„ ì—­ë¥˜", "ìœ„ê¶¤ì–‘", "ì‹­ì´ì§€ì¥ ê¶¤ì–‘", "ì†Œí™”ì„± ê¶¤ì–‘",
    "ìœ„ì¥ê´€ ì¶œí˜ˆ", "ì¥ íìƒ‰", "ëŒ€ì¥ ìš©ì¢…", "ëŒ€ì¥ í´ë¦½", "ê³¼ë¯¼ì„± ì¥ ì¦í›„êµ°",
    "í¬ë¡ ë³‘", "ê¶¤ì–‘ì„± ëŒ€ì¥ì—¼", "ì—¼ì¦ì„± ì¥ ì§ˆí™˜", "ë§Œì„± ì„¤ì‚¬", "ë³€ë¹„",
    "ê°‘ìƒì„  ê¸°ëŠ¥ í•­ì§„ì¦", "ê°‘ìƒì„  ê¸°ëŠ¥ ì €í•˜ì¦", "ê°‘ìƒì„ ì—¼", "í•˜ì‹œëª¨í†  ê°‘ìƒì„ ì—¼",
    "ê°‘ìƒì„  ê²°ì ˆ", "ê°‘ìƒì„  ì•”", "ë¶€ì‹  ê¸°ëŠ¥ ë¶€ì „", "ì¿ ì‹± ì¦í›„êµ°", "ê°ˆìƒ‰ì„¸í¬ì¢…",
    "ë¥˜ë§ˆí‹°ìŠ¤ ê´€ì ˆì—¼", "ê³¨ê´€ì ˆì—¼", "ê°•ì§ì„± ì²™ì¶”ì—¼", "í†µí’", "ì„¬ìœ ê·¼í†µ",
    "ê±´ì„ ", "ìŠµì§„", "ë‘ë“œëŸ¬ê¸°", "í¸ë‘í†µ", "ê¸´ì¥ì„± ë‘í†µ", "êµ°ë°œì„± ë‘í†µ",
    "ì‚¼ì°¨ì‹ ê²½í†µ", "ì•ˆë©´ ì‹ ê²½ ë§ˆë¹„", "ëŒ€ìƒ í¬ì§„", "ëŒ€ìƒ í¬ì§„ í›„ ì‹ ê²½í†µ",
    "ì²œì‹", "ë§Œì„± íì‡„ì„± íì§ˆí™˜", "ë§Œì„± ê¸°ê´€ì§€ì—¼", "íê¸°ì¢…", "íë ´",
    "íìƒ‰ì „ì¦", "íê²°í•µ", "ê°„ì§ˆì„± íì§ˆí™˜", "íì„¬ìœ ì¦", "ê¸°í‰",
    "ê°„ì—¼", "ê°„ê²½í™”", "ì§€ë°©ê°„", "ì•Œì½”ì˜¬ì„± ê°„ì§ˆí™˜", "ë¹„ì•Œì½”ì˜¬ì„± ì§€ë°©ê°„ì—¼",
    "ë‹´ì„ì¦", "ë‹´ë‚­ì—¼", "ë‹´ë‚­ í´ë¦½", "ì·Œì¥ì—¼", "ì·Œì¥ì•”",
    "ì‹ ì¥ì—¼", "ì‹ ìš°ì‹ ì—¼", "ì‹ ë¶€ì „", "ë§Œì„± ì‹ ì¥ë³‘", "ì‹ ì¦í›„êµ°",
    "ìš”ë¡œ ê°ì—¼", "ë°©ê´‘ì—¼", "ì „ë¦½ì„  ë¹„ëŒ€ì¦", "ì „ë¦½ì„ ì—¼", "ì „ë¦½ì„ ì•”",
    "ìœ ë°© ì„¬ìœ ì„ ì¢…", "ìœ ë°©ì—¼", "ìœ ë°©ì•”", "ìê¶ ê·¼ì¢…", "ìê¶ë‚´ë§‰ì¦",
    "ìê¶ê²½ë¶€ì—¼", "ìê¶ê²½ë¶€ì•”", "ë‚œì†Œë‚­ì¢…", "ë‚œì†Œì•”", "ì§ˆì—¼",
    "ìš°ìš¸ì¦", "ê³µí™© ì¥ì• ", "ë¶ˆì•ˆ ì¥ì• ", "ê°•ë°• ì¥ì• ", "ì™¸ìƒí›„ ìŠ¤íŠ¸ë ˆìŠ¤ ì¥ì• ",
    "ì–‘ê·¹ì„± ì¥ì• ", "ì •ì‹ ë¶„ì—´ì¦", "ì¹˜ë§¤", "ì•Œì¸ í•˜ì´ë¨¸ë³‘", "íŒŒí‚¨ìŠ¨ë³‘","ë¶€ë¶€ê´€ê³„",
    "ë‹¤ë°œì„± ê²½í™”ì¦", "ê·¼ìœ„ì¶•ì„± ì¸¡ì‚­ ê²½í™”ì¦", "í—ŒíŒ…í„´ë³‘", "ê¸¸ë­-ë°”ë ˆ ì¦í›„êµ°",
    "ì¤‘ì¦ ê·¼ë¬´ë ¥ì¦", "ê°„ì§ˆ", "ë‡Œì „ì¦", "í¸ë‘í†µ", "í˜„í›ˆì¦", "ë©”ë‹ˆì—ë¥´ë³‘","ë°œìŒì¥ì• ","ì˜ì‹ì¥ì• ",
    "ë°±ë‚´ì¥", "ë…¹ë‚´ì¥", "í™©ë°˜ë³€ì„±", "ë§ë§‰ë°•ë¦¬", "ê²°ë§‰ì—¼","í•œìª½ë§ˆë¹„","ê°ê°ì €í•˜",
    "ë¶€ë¹„ë™ì—¼", "í¸ë„ì—¼", "ì¸ë‘ì—¼", "í›„ë‘ì—¼", "ì¤‘ì´ì—¼","ê·¼ìœ„ì§€ê³¨",
    "ê°ê¸°", "ë…ê°", "íë ´", "ê¸°ê´€ì§€ì—¼", "í›„ë‘ì—¼","í˜¸í¡ê³¤ë€","ì¤‘ìˆ˜ê³¨","ì¤‘ì¡±ê³¨ì˜ ë‹¨ì¶•",
    "ì¶©ìˆ˜ì—¼", "ëŒ€ì¥ì—¼", "ê²Œì‹¤ì—¼", "ì·Œì¥ì—¼", "ë‹´ë‚­ì—¼","ì‘ì—´ê°","ê·¼ìœ¡ë§ˆë¹„","ì‹ ê²½ ë§ˆë¹„",
    "HIV ê°ì—¼", "ì—ì´ì¦ˆ", "ê²°í•µ", "ë§ë¼ë¦¬ì•„", "ë…ê¸°ì—´","ì‹œì•¼ì¥ì• ","ê·¼ìœ„ ì§€ê³¨ì˜ ë‹¨ì¶•",
    "ê³¨ì ˆ", "íƒˆêµ¬", "ì—¼ì¢Œ", "íƒ€ë°•ìƒ", "ì—´ìƒ","ì‡ëª¸ì¶œí˜ˆ","ì•ˆê²€í•˜ìˆ˜","ê¸°ì–µì¥ì• ","ì •ì‹  ì§€ì²´",
    "í™”ìƒ", "ë™ìƒ", "ì¼ì‚¬ë³‘", "ì—´ì‚¬ë³‘", "ê°ì „","í”¼í•˜ì¶œí˜ˆ","ì „í–¥ì„± ê¸°ì–µìƒì‹¤","í›„í–¥ì„± ê¸°ì–µìƒì‹¤","ì¤‘ìœ„ ì§€ê³¨ì˜ ë‹¨ì¶•",
    "COVID-19", "ì½”ë¡œë‚˜19", "ì¤‘ì¦ ê¸‰ì„± í˜¸í¡ê¸° ì¦í›„êµ°", "ì¤‘ë™ í˜¸í¡ê¸° ì¦í›„êµ°", "ì—ë³¼ë¼ ë°”ì´ëŸ¬ìŠ¤ë³‘","ì›ìœ„ ì§€ê³¨ì˜ ë‹¨ì¶•",
    "ê¸‰ì„± ì‹¬ê·¼ê²½ìƒ‰", "ê¸‰ì„± ì·Œì¥ì—¼", "ê¸‰ì„± ë‹´ë‚­ì—¼", "ê¸‰ì„± ì‹ ë¶€ì „", "ê¸‰ì„± í˜¸í¡ ê³¤ë€ ì¦í›„êµ°","ê¶Œíƒœê°","ì†Œí™”ê´€ ì¶œí˜ˆ",
]

class DataLoader:
    """ë°ì´í„° ë¡œë”© ë° ê²€ì¦ í´ë˜ìŠ¤"""
    
    @staticmethod
    def load_disease_data(file_path: str) -> pd.DataFrame:
        """ì§ˆë³‘ ë°ì´í„° CSV ë¡œë“œ"""
        try:
            df = pd.read_csv(file_path, encoding='utf-8')
            logger.info(f"ì§ˆë³‘ ë°ì´í„° ë¡œë“œ ì„±ê³µ: {len(df)}ê°œ ë ˆì½”ë“œ")
            
            # í•„ìš”í•œ ì»¬ëŸ¼ í™•ì¸
            required_cols = ['disnm_ko', 'disnm_en', 'dep', 'def', 'sym']
            missing_cols = [col for col in required_cols if col not in df.columns]
            if missing_cols:
                logger.error(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_cols}")
                return None
                
            return df
        except Exception as e:
            logger.error(f"ì§ˆë³‘ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    @staticmethod
    def load_medical_terms(file_path: str) -> Set[str]:
        """ì˜í•™ìš©ì–´ ì‚¬ì „ ë¡œë“œ"""
        medical_terms = set()
        
        try:
            if not os.path.exists(file_path):
                logger.warning(f"ì˜í•™ìš©ì–´ íŒŒì¼ ì—†ìŒ: {file_path}")
                return medical_terms
                
            df = pd.read_csv(file_path, encoding='utf-8')
            logger.info(f"ì˜í•™ìš©ì–´ ì‚¬ì „ ë¡œë“œ: {len(df)}ê°œ í–‰")
            
            # medterm ì»¬ëŸ¼ì—ì„œ í•œê¸€ ì˜í•™ìš©ì–´ ì¶”ì¶œ
            if 'medterm' in df.columns:
                for _, row in df.iterrows():
                    if pd.notna(row['medterm']):
                        term = str(row['medterm']).strip()
                        if term and len(term) > 1:
                            medical_terms.add(term)
                            # ë³µí•©ëª…ì‚¬ë„ ì¶”ê°€
                            if ' ' in term or '-' in term:
                                COMPOUND_NOUNS.append(term.replace('-', ' '))
            
            logger.info(f"ì˜í•™ìš©ì–´ {len(medical_terms)}ê°œ ë¡œë“œ ì™„ë£Œ")
            return medical_terms
            
        except Exception as e:
            logger.error(f"ì˜í•™ìš©ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return medical_terms

class TextPreprocessor:
    """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    @staticmethod
    def clean_text(text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if not isinstance(text, str):
            return ""
        
        # ê´„í˜¸ ë‚´ìš© ì œê±° (í•œê¸€ í¬í•¨ëœ ê´„í˜¸ëŠ” ë³´ì¡´)
        text = re.sub(r'\([^ê°€-í£]*\)', '', text)
        
        # ìˆ«ì + ë‹¨ìœ„ ì œê±°
        text = re.sub(r'\d+(?:mg|ml|íšŒ|ë²ˆ|ì‹œê°„|ì¼|ì£¼|ê°œì›”|ë…„|ë„|â„ƒ)', '', text, flags=re.IGNORECASE)
        
        # ìˆœìˆ˜ ìˆ«ì ì œê±°
        text = re.sub(r'\b\d+\b', '', text)
        
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ê³µë°±, í•˜ì´í”ˆë§Œ ë³´ì¡´)
        text = re.sub(r'[^\wê°€-í£ã„±-ã…ã…-ã…£\s\-]', ' ', text)
        
        # ì¤‘ë³µ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text

class SymptomTokenizer:
    """ì¦ìƒ ì¤‘ì‹¬ í† í°í™” í´ë˜ìŠ¤"""
    
    def __init__(self, medical_terms: Set[str]):
        """í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”"""
        self.tokenizer = Okt()
        self.medical_terms = medical_terms
        self.compound_nouns = set(COMPOUND_NOUNS)
        self.stopwords = set(STOPWORDS)
        
        # ì˜í•™ìš©ì–´ë¥¼ ë³µí•©ëª…ì‚¬ì—ë„ ì¶”ê°€
        for term in medical_terms:
            if ' ' in term or len(term) >= 3:
                self.compound_nouns.add(term)
        
        logger.info(f"í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”: ì˜í•™ìš©ì–´ {len(self.medical_terms)}ê°œ, ë³µí•©ëª…ì‚¬ {len(self.compound_nouns)}ê°œ")
    
    def extract_compound_terms(self, text: str) -> List[str]:
        """ë³µí•© ì˜í•™ìš©ì–´ ì¶”ì¶œ"""
        found_compounds = []
        
        # ê¸°ì¡´ ë³µí•©ëª…ì‚¬ ë§¤ì¹­ (ê¸´ ê²ƒë¶€í„° ìš°ì„ )
        sorted_compounds = sorted(self.compound_nouns, key=len, reverse=True)
        for compound in sorted_compounds:
            if compound in text and compound not in found_compounds:
                found_compounds.append(compound)
        
        # íŒ¨í„´ ê¸°ë°˜ ë³µí•©ìš©ì–´ ì¶”ì¶œ
        patterns = [
            r'([\wê°€-í£]+\s+(?:ì¦í›„êµ°|ì§ˆí™˜|ì¥ì• |ì¦ìƒ|ì§ˆë³‘|ì—¼ì¦|ê°ì—¼|ê²°ì„|ê¶¤ì–‘|ì•”))',
            r'((?:ê¸‰ì„±|ë§Œì„±|ì•Œë ˆë¥´ê¸°ì„±|ì—¼ì¦ì„±|ê°ì—¼ì„±)\s+[\wê°€-í£]+)',
            r'([\wê°€-í£]+ì„±\s+[\wê°€-í£]+)',
            r'(ì œ\d+í˜•\s+[\wê°€-í£]+)',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                if match not in found_compounds and len(match) > 3:
                    found_compounds.append(match)
        
        return found_compounds
    
    def extract_medical_terms(self, text: str) -> List[str]:
        """ì˜í•™ìš©ì–´ ì¶”ì¶œ"""
        found_terms = []
        
        # ì˜í•™ìš©ì–´ ì‚¬ì „ì—ì„œ ë§¤ì¹­
        for term in self.medical_terms:
            if term in text and term not in found_terms:
                found_terms.append(term)
        
        # ì˜í•™ íŒ¨í„´ ì¶”ì¶œ (ì¦ìƒ ê´€ë ¨)
        symptom_patterns = [
            r'([\wê°€-í£]{2,}í†µ)',      # ë‘í†µ, ë³µí†µ, ìš”í†µ ë“±
            r'([\wê°€-í£]{2,}ì—´)',      # ë°œì—´, ë¯¸ì—´ ë“±
            r'([\wê°€-í£]{2,}ì¦)',      # ì–´ì§€ëŸ¼ì¦, ìš¸í˜ˆì¦ ë“±
            r'([\wê°€-í£]{2,}ì—¼)',      # ìœ„ì—¼, ê°„ì—¼, íë ´ ë“±
            r'([\wê°€-í£]{2,}ë³‘)',      # ë‹¹ë‡¨ë³‘, ê³ í˜ˆì•• ë“±
        ]
        
        for pattern in symptom_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                if match not in found_terms and len(match) >= 2:
                    found_terms.append(match)
        
        return found_terms
    
    def is_valid_token(self, token: str) -> bool:
        """í† í° ìœ íš¨ì„± ê²€ì‚¬"""
        # ì˜í•™ìš©ì–´ë‚˜ ë³µí•©ëª…ì‚¬ë©´ ìœ íš¨
        if token in self.medical_terms or token in self.compound_nouns:
            return True
        
        # ë¶ˆìš©ì–´ë©´ ë¬´íš¨
        if token in self.stopwords:
            return False
        
        # ê¸¸ì´ ì²´í¬
        if len(token) < 2:
            return False
        
        # í•œê¸€ ëª…ì‚¬ì¸ì§€ í™•ì¸
        if not re.match(r'^[ê°€-í£]+$', token):
            return False
        
        return True
    
    def tokenize_symptoms(self, symptoms_text: str) -> List[str]:
        """ì¦ìƒ í…ìŠ¤íŠ¸ í† í°í™”"""
        if not symptoms_text or not isinstance(symptoms_text, str):
            return []
        
        # ì „ì²˜ë¦¬
        cleaned_text = TextPreprocessor.clean_text(symptoms_text)
        if not cleaned_text:
            return []
        
        # 1. ë³µí•©ìš©ì–´ ì¶”ì¶œ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
        compound_terms = self.extract_compound_terms(cleaned_text)
        
        # 2. ì˜í•™ìš©ì–´ ì¶”ì¶œ
        medical_terms = self.extract_medical_terms(cleaned_text)
        
        # 3. ì¶”ì¶œëœ ìš©ì–´ë“¤ì„ ì„ì‹œ ì œê±°í•˜ì—¬ ì¤‘ë³µ ë°©ì§€
        temp_text = cleaned_text
        for term in compound_terms + medical_terms:
            temp_text = re.sub(rf'\b{re.escape(term)}\b', ' ', temp_text)
        temp_text = re.sub(r'\s+', ' ', temp_text).strip()
        
        # 4. ë‚˜ë¨¸ì§€ í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬ ì¶”ì¶œ
        nouns = []
        if temp_text:
            nouns = self.tokenizer.nouns(temp_text)
        
        # 5. ëª¨ë“  í† í° í•©ì¹˜ê¸°
        all_tokens = compound_terms + medical_terms + nouns
        
        # 6. í•„í„°ë§ ë° ì¤‘ë³µ ì œê±°
        final_tokens = []
        for token in all_tokens:
            if token and token not in final_tokens and self.is_valid_token(token):
                final_tokens.append(token)
        
        return final_tokens

class QualityManager:
    """í† í°í™” í’ˆì§ˆ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    @staticmethod
    def calculate_statistics(results: List[Dict]) -> Dict:
        """í† í°í™” ê²°ê³¼ í†µê³„ ê³„ì‚°"""
        if not results:
            return {}
        
        token_counts = []
        total_tokens = []
        
        for result in results:
            if 'tokens' in result and isinstance(result['tokens'], list):
                token_count = len(result['tokens'])
                token_counts.append(token_count)
                total_tokens.extend(result['tokens'])
        
        unique_tokens = set(total_tokens)
        token_freq = Counter(total_tokens)
        
        stats = {
            'total_records': len(results),
            'avg_tokens_per_record': np.mean(token_counts) if token_counts else 0,
            'max_tokens': max(token_counts) if token_counts else 0,
            'min_tokens': min(token_counts) if token_counts else 0,
            'total_unique_tokens': len(unique_tokens),
            'total_tokens': len(total_tokens),
            'top_10_tokens': token_freq.most_common(10)
        }
        
        return stats
    
    @staticmethod
    def generate_report(stats: Dict, output_path: str):
        """í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = f"""
=== í† í°í™” í’ˆì§ˆ ë¦¬í¬íŠ¸ ===
ìƒì„±ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

ğŸ“Š ê¸°ë³¸ í†µê³„:
- ì²˜ë¦¬ëœ ë ˆì½”ë“œ ìˆ˜: {stats['total_records']}ê°œ
- ë ˆì½”ë“œë‹¹ í‰ê·  í† í° ìˆ˜: {stats['avg_tokens_per_record']:.2f}ê°œ
- ìµœëŒ€ í† í° ìˆ˜: {stats['max_tokens']}ê°œ
- ìµœì†Œ í† í° ìˆ˜: {stats['min_tokens']}ê°œ
- ì´ ê³ ìœ  í† í° ìˆ˜: {stats['total_unique_tokens']}ê°œ
- ì´ í† í° ìˆ˜: {stats['total_tokens']}ê°œ

ğŸ” ë¹ˆì¶œ í† í° Top 10:
"""
        for i, (token, count) in enumerate(stats['top_10_tokens'], 1):
            report += f"{i:2d}. {token} ({count}íšŒ)\n"
        
        report_path = output_path.replace('.csv', '_report.txt')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"í’ˆì§ˆ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
        print(report)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("=== ì¦ìƒ ì¤‘ì‹¬ í† í°í™” ì‹œì‘ ===")
    
    # 1. ë°ì´í„° ë¡œë“œ
    logger.info("1. ë°ì´í„° ë¡œë”©...")
    df = DataLoader.load_disease_data(DISEASE_CSV_PATH)
    if df is None:
        logger.error("ì§ˆë³‘ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
        return
    
    medical_terms = DataLoader.load_medical_terms(MED_TERMS_CSV_PATH)
    
    # 2. í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
    logger.info("2. í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”...")
    tokenizer = SymptomTokenizer(medical_terms)
    
    # 3. ì¦ìƒì´ ìˆëŠ” ë ˆì½”ë“œë§Œ í•„í„°ë§
    symptom_df = df[df['sym'].notna() & (df['sym'].str.strip() != '')].copy()
    logger.info(f"ì¦ìƒ ì •ë³´ ìˆëŠ” ë ˆì½”ë“œ: {len(symptom_df)}ê°œ (ì „ì²´ {len(df)}ê°œ ì¤‘ {len(symptom_df)/len(df)*100:.1f}%)")
    
    # 4. í† í°í™” ì²˜ë¦¬
    logger.info("3. í† í°í™” ì²˜ë¦¬ ì‹œì‘...")
    results = []
    
    for idx, row in symptom_df.iterrows():
        try:
            # ì¦ìƒ í…ìŠ¤íŠ¸ í† í°í™”
            symptoms = str(row['sym']) if pd.notna(row['sym']) else ""
            tokens = tokenizer.tokenize_symptoms(symptoms)
            
            # ê²°ê³¼ ë ˆì½”ë“œ ìƒì„±
            result = {
                'id': idx,
                'disnm_ko': str(row.get('disnm_ko', '')),
                'disnm_en': str(row.get('disnm_en', '')),
                'dep': str(row.get('dep', '')),
                'def': str(row.get('def', '')),
                'symptoms': symptoms,
                'therapy': str(row.get('therapy', '')),
                'tokens': tokens,
                'def_k': ' '.join(tokens),  # TF-IDFìš© ë¬¸ìì—´
                'tokens_json': json.dumps(tokens, ensure_ascii=False)  # CSV ì €ì¥ìš©
            }
            results.append(result)
            
            # ì§„í–‰ìƒí™© ë¡œê¹…
            if (len(results)) % 100 == 0:
                logger.info(f"ì²˜ë¦¬ ì§„í–‰: {len(results)}/{len(symptom_df)} ({len(results)/len(symptom_df)*100:.1f}%)")
                
        except Exception as e:
            logger.error(f"ë ˆì½”ë“œ {idx} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    
    # 5. ê²°ê³¼ ì €ì¥
    logger.info("4. ê²°ê³¼ ì €ì¥...")
    result_df = pd.DataFrame(results)
    result_df.to_csv(OUTPUT_CSV_PATH, index=False, encoding='utf-8')
    logger.info(f"âœ… í† í°í™” ê²°ê³¼ ì €ì¥: {OUTPUT_CSV_PATH}")
    logger.info(f"âœ… ì²˜ë¦¬ ì™„ë£Œ: {len(results)}ê°œ ë ˆì½”ë“œ")
    
    # 6. í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„±
    logger.info("5. í’ˆì§ˆ ë¶„ì„...")
    stats = QualityManager.calculate_statistics(results)
    QualityManager.generate_report(stats, OUTPUT_CSV_PATH)
    
    logger.info("=== í† í°í™” ì™„ë£Œ ===")

if __name__ == "__main__":
    main()