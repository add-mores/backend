# backend/code/disease/disease_vector/verify_vectors.py
# ë””ë ‰í† ë¦¬: backend/code/disease/disease_vector

import os
import json
import numpy as np
import pandas as pd
from sqlalchemy import create_engine, text
from sklearn.metrics.pairwise import cosine_similarity
from dotenv import load_dotenv
import logging

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class VectorVerifier:
    def __init__(self, db_url: str):
        self.engine = create_engine(db_url)
        
    def verify_basic_stats(self):
        """ê¸°ë³¸ í†µê³„ í™•ì¸"""
        print("=== ê¸°ë³¸ í†µê³„ í™•ì¸ ===")
        
        query = """
        SELECT 
            (SELECT COUNT(*) FROM tfidf_metadata) as metadata_count,
            (SELECT COUNT(*) FROM disease_vectors) as vector_count,
            (SELECT COUNT(*) FROM vectorization_logs) as log_count;
        """
        
        result = pd.read_sql(query, self.engine)
        print(f"ë©”íƒ€ë°ì´í„° í…Œì´ë¸”: {result.iloc[0]['metadata_count']}ê°œ")
        print(f"ë²¡í„° í…Œì´ë¸”: {result.iloc[0]['vector_count']}ê°œ")
        print(f"ë¡œê·¸ í…Œì´ë¸”: {result.iloc[0]['log_count']}ê°œ")
        print()
        
    def verify_metadata(self):
        """ë©”íƒ€ë°ì´í„° ì •ë³´ í™•ì¸"""
        print("=== ë©”íƒ€ë°ì´í„° ì •ë³´ ===")
        
        query = """
        SELECT id, feature_count, min_df, max_df, max_features, 
               ngram_range_min, ngram_range_max, description, created_at
        FROM tfidf_metadata 
        ORDER BY id DESC LIMIT 1;
        """
        
        result = pd.read_sql(query, self.engine)
        for _, row in result.iterrows():
            print(f"ë©”íƒ€ë°ì´í„° ID: {row['id']}")
            print(f"ì–´íœ˜ ì‚¬ì „ í¬ê¸°: {row['feature_count']}")
            print(f"ìµœì†Œ ë¬¸ì„œ ë¹ˆë„: {row['min_df']}")
            print(f"ìµœëŒ€ ë¬¸ì„œ ë¹ˆë„: {row['max_df']}")
            print(f"ìµœëŒ€ íŠ¹ì„± ìˆ˜: {row['max_features']}")
            print(f"N-gram ë²”ìœ„: {row['ngram_range_min']}-{row['ngram_range_max']}")
            print(f"ì„¤ëª…: {row['description']}")
            print(f"ìƒì„±ì¼: {row['created_at']}")
        print()
        
    def verify_vector_quality(self):
        """ë²¡í„° í’ˆì§ˆ í™•ì¸"""
        print("=== ë²¡í„° í’ˆì§ˆ í†µê³„ ===")
        
        query = """
        SELECT 
            COUNT(*) as total_vectors,
            MIN(vector_norm) as min_norm,
            MAX(vector_norm) as max_norm,
            AVG(vector_norm) as avg_norm,
            STDDEV(vector_norm) as std_norm,
            MIN(non_zero_count) as min_features,
            MAX(non_zero_count) as max_features,
            AVG(non_zero_count) as avg_features
        FROM disease_vectors;
        """
        
        result = pd.read_sql(query, self.engine)
        for _, row in result.iterrows():
            print(f"ì „ì²´ ë²¡í„° ìˆ˜: {row['total_vectors']}")
            print(f"ë²¡í„° ë…¸ë¦„ - ìµœì†Œ: {row['min_norm']:.4f}, ìµœëŒ€: {row['max_norm']:.4f}, í‰ê· : {row['avg_norm']:.4f}")
            print(f"0ì´ ì•„ë‹Œ íŠ¹ì„± ìˆ˜ - ìµœì†Œ: {row['min_features']}, ìµœëŒ€: {row['max_features']}, í‰ê· : {row['avg_features']:.1f}")
        print()
        
    def sample_vectors(self, n=5):
        """ìƒ˜í”Œ ë²¡í„° í™•ì¸"""
        print(f"=== ìƒ˜í”Œ ë²¡í„° í™•ì¸ (ìƒìœ„ {n}ê°œ) ===")
        
        query = f"""
        SELECT disease_id, disease_name_ko, department, 
               vector_norm, non_zero_count
        FROM disease_vectors 
        ORDER BY disease_id 
        LIMIT {n};
        """
        
        result = pd.read_sql(query, self.engine)
        for _, row in result.iterrows():
            print(f"ID: {row['disease_id']}")
            print(f"ì§ˆë³‘ëª…: {row['disease_name_ko']}")
            print(f"ì§„ë£Œê³¼: {row['department']}")
            print(f"ë²¡í„° ë…¸ë¦„: {row['vector_norm']:.4f}")
            print(f"íŠ¹ì„± ìˆ˜: {row['non_zero_count']}")
            print("-" * 50)
        print()
        
    def test_similarity_calculation(self):
        """ìœ ì‚¬ë„ ê³„ì‚° í…ŒìŠ¤íŠ¸"""
        print("=== ìœ ì‚¬ë„ ê³„ì‚° í…ŒìŠ¤íŠ¸ ===")
        
        # ì„ì˜ì˜ ë‘ ì§ˆë³‘ ë²¡í„° ê°€ì ¸ì˜¤ê¸°
        query = """
        SELECT disease_id, disease_name_ko, tfidf_vector
        FROM disease_vectors 
        WHERE vector_norm > 0
        ORDER BY disease_id 
        LIMIT 3;
        """
        
        result = pd.read_sql(query, self.engine)
        
        vectors = []
        disease_names = []
        
        for _, row in result.iterrows():
            # JSONì—ì„œ ë²¡í„° ë³µì›
            vector_data = row['tfidf_vector']
            
            # ì´ë¯¸ dict í˜•íƒœì¸ì§€ í™•ì¸
            if isinstance(vector_data, dict):
                sparse_vector = vector_data
            else:
                # ë¬¸ìì—´ì¸ ê²½ìš° JSON íŒŒì‹±
                sparse_vector = json.loads(vector_data)
            
            # 5000ì°¨ì› ë²¡í„°ë¡œ ë³µì› (0ìœ¼ë¡œ ì´ˆê¸°í™”)
            full_vector = np.zeros(5000)
            for idx, value in sparse_vector.items():
                full_vector[int(idx)] = value
                
            vectors.append(full_vector)
            disease_names.append(row['disease_name_ko'])
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        if len(vectors) >= 2:
            similarity_matrix = cosine_similarity(vectors)
            
            print("ì§ˆë³‘ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„:")
            for i, name1 in enumerate(disease_names):
                for j, name2 in enumerate(disease_names):
                    if i != j:
                        print(f"{name1} vs {name2}: {similarity_matrix[i][j]:.4f}")
        print()
        
    def verify_vocabulary_sample(self):
        """ì–´íœ˜ ì‚¬ì „ ìƒ˜í”Œ í™•ì¸"""
        print("=== ì–´íœ˜ ì‚¬ì „ ìƒ˜í”Œ ===")
        
        query = "SELECT vocabulary FROM tfidf_metadata ORDER BY id DESC LIMIT 1;"
        result = pd.read_sql(query, self.engine)
        
        if not result.empty:
            vocab_data = result.iloc[0]['vocabulary']
            
            # ì´ë¯¸ dict í˜•íƒœì¸ì§€ í™•ì¸
            if isinstance(vocab_data, dict):
                vocabulary = vocab_data
            else:
                # ë¬¸ìì—´ì¸ ê²½ìš° JSON íŒŒì‹±
                vocabulary = json.loads(vocab_data)
            
            # ìƒ˜í”Œ ë‹¨ì–´ 10ê°œ í‘œì‹œ
            sample_words = list(vocabulary.items())[:10]
            print("ì–´íœ˜ ì‚¬ì „ ìƒ˜í”Œ (ë‹¨ì–´: ì¸ë±ìŠ¤):")
            for word, idx in sample_words:
                print(f"  {word}: {idx}")
            
            print(f"\nì´ ì–´íœ˜ ìˆ˜: {len(vocabulary)}")
        print()
        
    def check_department_distribution(self):
        """ì§„ë£Œê³¼ë³„ ë¶„í¬ í™•ì¸"""
        print("=== ì§„ë£Œê³¼ë³„ ì§ˆë³‘ ë¶„í¬ ===")
        
        query = """
        SELECT department, COUNT(*) as disease_count
        FROM disease_vectors 
        WHERE department IS NOT NULL
        GROUP BY department 
        ORDER BY disease_count DESC 
        LIMIT 10;
        """
        
        result = pd.read_sql(query, self.engine)
        for _, row in result.iterrows():
            print(f"{row['department']}: {row['disease_count']}ê°œ")
        print()
        
    def run_all_verifications(self):
        """ëª¨ë“  ê²€ì¦ ì‹¤í–‰"""
        print("ğŸ” TF-IDF ë²¡í„°í™” ê²°ê³¼ ê²€ì¦ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n")
        
        try:
            self.verify_basic_stats()
            self.verify_metadata()
            self.verify_vector_quality()
            self.sample_vectors()
            self.verify_vocabulary_sample()
            self.check_department_distribution()
            self.test_similarity_calculation()
            
            print("âœ… ëª¨ë“  ê²€ì¦ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            print("ë²¡í„°í™”ê°€ ì •ìƒì ìœ¼ë¡œ ìˆ˜í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"âŒ ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            logger.error(f"ê²€ì¦ ì‹¤íŒ¨: {e}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    database_url = os.getenv("DATABASE_URL")
    if not database_url:
        raise ValueError("DATABASE_URL í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    verifier = VectorVerifier(database_url)
    verifier.run_all_verifications()


if __name__ == "__main__":
    main()