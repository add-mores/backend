import pandas as pd
import requests
import os
import concurrent.futures
import logging
import time
import random
from dotenv import load_dotenv
from tqdm import tqdm

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
KAKAO_API_KEY = os.getenv("KAKAO_API_KEY")
headers = {"Authorization": f"KakaoAK {KAKAO_API_KEY}"}
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë‹¨ì¼ ì£¼ì†Œ ìœ„ê²½ë„ ìš”ì²­ í•¨ìˆ˜ (ì¬ì‹œë„ í¬í•¨ + ë””ë²„ê¹… ë¡œê·¸) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_lat_lon_retry(address, retry=3):
    for attempt in range(retry):
        try:
            time.sleep(random.uniform(0.1, 0.3))  # API ê³¼ë¶€í•˜ ë°©ì§€
            url = "https://dapi.kakao.com/v2/local/search/address.json"
            params = {"query": address}
            res = requests.get(url, headers=headers, params=params, timeout=5)

            if res.status_code != 200:
                logging.warning(f"âš ï¸ [ì‘ë‹µì½”ë“œ {res.status_code}] {address}")
                continue

            res_json = res.json()

            if "documents" not in res_json:
                logging.warning(f"ğŸ“› 'documents' í‚¤ ì—†ìŒ: {address} â†’ ì‘ë‹µ: {res_json}")
                continue

            if not res_json["documents"]:
                logging.warning(f"ğŸ” ì£¼ì†Œ ë¯¸ë§¤ì¹­: {address} â†’ ì‘ë‹µ ìˆìŒ but ê²°ê³¼ ì—†ìŒ")
                continue

            x = float(res_json["documents"][0]["x"])
            y = float(res_json["documents"][0]["y"])
            return y, x

        except Exception as e:
            logging.error(f"âŒ [ì¬ì‹œë„ {attempt+1}/{retry}] {address} â†’ ì—ëŸ¬: {e}")

    logging.error(f"ğŸš« ìµœì¢… ì‹¤íŒ¨: {address}")
    return None, None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë³‘ë ¬ ì¬ìš”ì²­ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parallel_geocode_retry(address_list, max_workers=15):
    results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(get_lat_lon_retry, addr) for addr in address_list]
        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):
            results.append(future.result())
    return results

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì‹¤íŒ¨ ì£¼ì†Œ ë¶ˆëŸ¬ì˜¤ê¸° ë° ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
fail_df = pd.read_csv("../../pages/missing_hospitals.csv")
addresses = fail_df["address"].dropna().unique().tolist()

logging.info(f"ğŸš€ ì¬ì²˜ë¦¬ ì‹œì‘: ì‹¤íŒ¨ ì£¼ì†Œ {len(addresses)}ê°œ")

coords = parallel_geocode_retry(addresses)
fail_df["lat"], fail_df["lon"] = zip(*coords)

# ë³µêµ¬ëœ ì£¼ì†Œ ì €ì¥
recovered = fail_df.dropna(subset=["lat", "lon"])
recovered.to_csv("../../pages/recovered_coordinates.csv", index=False)

# ì‹¤íŒ¨í•œ ê²ƒë“¤ ë³„ë„ë¡œ ì €ì¥
still_failed = fail_df[fail_df["lat"].isna()]
still_failed.to_csv("still_failed.csv", index=False)

# ê²°ê³¼ ìš”ì•½
logging.info(f"âœ… ë³µêµ¬ ì™„ë£Œ: {len(recovered)}ê°œ ë³µêµ¬ / {len(fail_df)}ê°œ ì¤‘")
logging.info(f"âŒ ì—¬ì „íˆ ì‹¤íŒ¨: {len(still_failed)}ê°œ â†’ still_failed.csv ì €ì¥")

